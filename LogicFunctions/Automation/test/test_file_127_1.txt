Last Updated on August 20, 2020What is supervised machine learning and how does it relate to unsupervised machine learning?In this post you will discover supervised learning, unsupervised learning and semi-supervised learning. After reading this post you will know:Kick-start your project with my new book Master Machine Learning Algorithms, including step-by-step tutorials and the Excel Spreadsheet files for all examples.Let’s get started.Supervised and Unsupervised Machine Learning AlgorithmsPhoto by US Department of Education, some rights reserved.The majority of practical machine learning uses supervised learning.Supervised learning is where you have input variables (x) and an output variable (Y) and you use an algorithm to learn the mapping function from the input to the output.Y = f(X)The goal is to approximate the mapping function so well that when you have new input data (x) that you can predict the output variables (Y) for that data.It is called supervised learning because the process of an algorithm learning from the training dataset can be thought of as a teacher supervising the learning process. We know the correct answers, the algorithm iteratively makes predictions on the training data and is corrected by the teacher. Learning stops when the algorithm achieves an acceptable level of performance.Sample of the handy machine learning algorithms mind map.I've created a handy mind map of 60+ algorithms organized by type.Download it, print it and use it. Also get exclusive access to the machine learning algorithms email mini-course.  Supervised learning problems can be further grouped into regression and classification problems.Some common types of problems built on top of classification and regression include recommendation and time series prediction respectively.Some popular examples of supervised machine learning algorithms are:Unsupervised learning is where you only have input data (X) and no corresponding output variables.The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.These are called unsupervised learning because unlike supervised learning above there is no correct answers and there is no teacher. Algorithms are left to their own devises to discover and present the interesting structure in the data.Unsupervised learning problems can be further grouped into clustering and association problems.Some popular examples of unsupervised learning algorithms are:Problems where you have a large amount of input data (X) and only some of the data is labeled (Y) are called semi-supervised learning problems.These problems sit in between both supervised and unsupervised learning.A good example is a photo archive where only some of the images are labeled, (e.g. dog, cat, person) and the majority are unlabeled.Many real world machine learning problems fall into this area. This is because it can be expensive or time-consuming to label data as it may require access to domain experts. Whereas unlabeled data is cheap and easy to collect and store.You can use unsupervised learning techniques to discover and learn the structure in the input variables.You can also use supervised learning techniques to make best guess predictions for the unlabeled data, feed that data back into the supervised learning algorithm as training data and use the model to make predictions on new unseen data.In this post you learned the difference between supervised, unsupervised and semi-supervised learning. You now know that:Do you have any questions about supervised, unsupervised or semi-supervised learning? Leave a comment and ask your question and I will do my best to answer it....with just arithmetic and simple examplesDiscover how in my new Ebook: 
Master Machine Learning AlgorithmsIt covers explanations and examples of 10 top algorithms, like:
Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more...Skip the Academics. Just Results.Thanks for this post. That was helpful. My question is how does one determine the correct algorithm to use for a particular problem in supervised learning? Also,can a network trained by unsupervised learning be tested with new set of data (testing data) or its just for the purpose of grouping?Hi Omot, it is a good idea to try a suite of standard algorithms on your problem and discover what algorithm performs best.Normally, an unsupervised method is applied to all data available in order to learn something about that data and the broader problem. You could say cluster a “training” dataset and later see what clusters new data is closest to if you wanted to avoid re-clustering the data.sir, does k-means clustering can be implemented in MATLAB to predict the data for unsupervised learning.k-means is a clustering algorithm. It is not used to make predictions, instead it is used to group data. Learn more here:
https://en.wikipedia.org/wiki/K-means_clusteringHello Could clustering be used to create a dependent categorical variable from a number of numerical independent variables?
I am faced with a problem where i have a dataset with multiple independent numerical columns but i am not sure whether the dependent variable is correct.Sure. Try it and see if it helps.Hi, Sabarish v!
here you can better understand about k-algorithm, explained very wellhttps://blog.carbonteq.com/practical-image-recognition-with-tensorflow/Thanks for sharing.Which of the following is a supervised learning problem?
 A) Grouping people in a social network.
 B) Predicting credit approval based on historical data
 C) Predicting rainfall based on historical data
 D) all of the aboveI’d rather not do your homework for you.This framework can help you figure whether any problem is a supervised learning problem:
http://machinelearningmastery.com/how-to-define-your-machine-learning-problem/BB and CB and  CI need help in solving a problem. I have utilized all resources available and the school can’t find a tutor in this subject.  My question is this: What is the best method to choose if you want to train an algorithm that can discriminate between patients with hypertension and patients with hypertension and diabetes. Please help me understand!Hi Angel, this sounds like a problem specific problem. In general, we cannot know which data representation is best or which algorithm is best, they must be discovered empirically:
http://machinelearningmastery.com/a-data-driven-approach-to-machine-learning/I teach a process for working through predictive modeling problems methodically that you may find useful:
http://machinelearningmastery.com/start-here/#processvery informing article that tells differences between supervised and unsupervised learning!
thanks!Thanks.You can optimize your algorithm or compare between algorithms using Cross validation which in the case of supervised learning tries to find the best data to use for training and testing the algorithm.This content is really helpful. Can you give some examples of all these techniques with best description?? or a brief introduction of Reinforcement learning with example??Take a look at this post for a good list of algorithms:
http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/Hi Jason, Thank you for summary on types of ML algorithms
How can one use clustering or unsupervised learning for prediction on a new data. I have clustered the input data into clusters using hierarchical clustering, Now I want to check the membership of new data with the identified clusters. How is it possible. Is there an algorithm available in R?Hi Naveen, generally I don’t use unsupervised methods much as I don’t get much value from them in practice.You can use the cluster number, cluster centroid or other details as an input for modeling.Could you please give me a real world example of supervised, unsupervised, and semi supervised learning?Hi Tashrif,Supervised would be when you have a ton of labeled pictures of dogs and cats and you want to automatically label new pictures of dogs and cats.Unsupervised would be when you want to see how the pictures structurally relate to each other by color or scene or whatever.Semi-supervised is where you have a ton of pictures and only some are labelled and you want to use the unlabeled and the labelled to help you in turn label new pictures in the future.This was a really good read, so thanks for writing and publishing it.Question for you.  I have constructed a Random Forest model, so I’m using supervised learning, and I’m being asked to run an unlabeled data set through it.  But I won’t have the actual results of this model, so I can’t determine accuracy on it until I have the actual result of it.So my question is… how can I run a set of data through a ML model if I don’t have labels for it?  For further clarity and context, I’m running a random forest model to predict a binary classification label.  I get the first few data points relatively quickly, but the label takes 30 days to become clear.Maybe none of this makes sense, but I appreciate any direction you could possibly give.Many thanks,FrankThanks Frank. Great question.You will need to collect historical data to develop and evaluate your model.Once created, it sounds like you will need to wait 30 days before you can evaluate the ongoing performance of the model’s predictions.Hi Jason,
Have done a program to classify if a customer(client) will subscribe for term deposit or not..
dataset used: bank dataset from uci machine learning repository
algorithm used: 1. random forest algorithm with CART to generate decision trees and 2.random forest algorithm with HAC4.5 to generate decision trees.my question is how do i determine the accuracy of 1 and 2 and find the best one???am really new to this field..please ignore my stupidity
thanks in advanceHi Ann, great work!You can compare each algorithm using a consistent testing methodology. For example k-fold cross validation with the same random number seeds (so each algorithm gets the same folds).Here is more info on comparing algorithms:
http://machinelearningmastery.com/how-to-evaluate-machine-learning-algorithms/I hope that helps as a start.Hi Jason, greater work you are making I wish you the best you deserving it.My question: I want to use ML to solve problems of network infrastructure data information. You know missing, typo, discrepancy. Fundamentals in knowledge and expertise are essential though need some ML direction and research more. Can you provide or shed light off that? And how? If you prefer we can communicate directly at [email protected] Thanks and please forgive me if the approach seems awkward as startup and recently joint your connections it’s may be rushing!Hi Nihad, that is an interesting application.Machine learning might not be the best approach for fixing typos and such. Nevertheless, the first step would be to collect a dataset and try to deeply understand the types of examples the algorithm would have to learn.This post might help you dive deeper into your problem:
http://machinelearningmastery.com/how-to-define-your-machine-learning-problem/I hope this helps as a start, best of luck.Thanks for the tutorial , have been implementing your machine learning master to law on the Casebook Web Application built for lawyers,paralegals & law students.You are very welcome Nihad!  Thank you for letting me know about your application and how you have made use of our materials!Regards,Splendid work! A helpful measure for my semester exams. Thanks!!Thanks Nischay.hello Jason, greater work you are making I wish you the best you deserving it.
I want to find an online algorithm to cluster scientific workflow data to minimize run time and system overhead so it can map these workflow tasks to a distributed resources like clouds .The clustered data should be mapped to these available resources in a balanced way that  guarantees no resource is over utilized while other resource is idle. I came a cross a horizontal clustering ,vertical clustering but these technique  are static and user should determine the number of clusters and number of tasks in each cluster in advance …Hi Sam, Thanks for your support.Off-the-cuff, this sounds like a dynamic programming or constraint satisfaction problem rather than machine learning.Hi Jason, this post is really helpful for my Cognitive Neural Network revision!I have a question of a historical nature, relating to how supervised learning algorithms evolved:
Some early supervised learning methods allowed the threshold to be adjusted during learning. Why is that not necessary with the newer supervised learning algorithms?Is this because they (e.g. the Delta Rule) adjust the weights on a running basis to minimize error, which supersedes the need for threshold adjustment? Or is there something more subtle going on in the newer algorithms that eliminates the need for threshold adjustment? Thank you in advance for any insight you can provide on this.I don’t think I have enough context Marcus. It sounds like you may be referring specifically to stochastic gradient descent.I’m not really an algorithm historian, I’d refer you to the seminal papers on the topic.Hi Jason – Thanks so much for the informative post.  I think I am missing something basic.  Once a model is trained with labeled data (supervised), how does additional unlabeled data help improve the model?  For example, how do newly uploaded pictures (presumably unlabeled) to Google Photos help further improve the model (assuming it does so)?  Or how does new voice data (again unlabeled) help make a machine learning-based voice recognition system better? i understand conceptually how labeled data could drive a model but unclear how it helps if you don’t really know what the data represents.   Thanks! DaveGreat question Dave.Generally, we can use unlabelled data to help initialize large models, like deep neural networks.More specifically, we can label unlabelled data, have it corroborate the prediction if needed, and use that as input to update or retrain a model to make be better for future predictions.Does that help?yes thanks.  So the data ultimately needs to be labeled to be useful in improving the model?  Keeping with the Google Photos use case, all the millions of photos uploaded everyday then doesn’t help the model unless someone manually labels them and then runs those through the training?  Guess I was hoping there was some way intelligence could be discerned from the unlabeled data (unsupervised) to improve on the original model but that does not appear to be the case right?  thanks again for the help – DaveThere very well may be, I’m just not across it.For a business which uses machine learning, would it be correct to think that there are employees who manually label unlabeled data to overcome the problem raised by Dave? The amount of unlabeled data in such cases would be much smaller than all the photos in Google Photos.It is a good approach, e.g. to use local or remote labor to prepare/label a first-cut dataset.Can you write a blog post on Reinforcement Learning explaining how does it work, in context of Robotics ?I hope to cover the topic in the future Rohit.Hi Jason,I am trying to solve machine learning problem for Incidents in Health & safety industry.
I want to recommend the corrective or preventive actions based on the Incident happening at given site.
I am trying to understand which algorithm works best for this.
Could you please share your thoughts.Regards,
HaArmstrong was born near Wapakoneta, Ohio,[1] on August 5, 1930, the son of Viola Louise (née Engel) and Stephen Koenig Armstrong. He was of German, Scots-Irish, and Scottish descent.[2][3] He had a younger sister, June, and a younger brother, Dean. His father was an auditor for the Ohio state government,[4] and the family moved around the state repeatedly, living in 16 towns over the next 14 years.[5] Armstrong's love for flying grew during this time, having started at the age of two when his father took him to the Cleveland Air Races. When he was five or six, he experienced his first airplane flight in Warren, Ohio, when he and his father took a ride in a Ford Trimotor (also known as the "Tin Goose").[6][7]

The family's last move was in 1944 and took them back to Wapakoneta, where Armstrong attended Blume High School and took flying lessons at the Wapakoneta airfield.[1] He earned a student flight certificate on his 16th birthday, then soloed in August, all before he had a driver's license.[8] He was an active Boy Scout and earned the rank of Eagle Scout.[9] As an adult, he was recognized by the Scouts with their Distinguished Eagle Scout Award and Silver Buffalo Award.[10][11] While flying toward the Moon on July 18, 1969, he sent his regards to attendees at the National Scout jamboree in Idaho.[12] Among the few personal items that he carried with him to the Moon and back was a World Scout Badge.[13]

At age 17, in 1947, Armstrong began studying aeronautical engineering at Purdue University in West Lafayette, Indiana. He was the second person in his family to attend college. He was also accepted to the Massachusetts Institute of Technology (MIT),[14] but he resolved to go to Purdue after watching a football game between the Purdue Boilermakers and the Ohio State Buckeyes at the Ohio Stadium in 1945, in which quarterback Bob DeMoss led the Boilermakers to a sound victory over the highly regarded Buckeyes.[15] An uncle who attended MIT had also advised him that he could receive a good education without going all the way to Cambridge, Massachusetts. His college tuition was paid for under the Holloway Plan. Successful applicants committed to two years of study, followed by two years of flight training and one year of service as an aviator in the U.S. Navy, then completion of the final two years of their bachelor's degree.[14] Armstrong did not take courses in naval science, nor did he join the Naval Reserve Officers Training Corps.[16]

Navy service
A black-and-white image of a light-skinned man in his early 20s. He is looking off to his right. He has mid-colored hair parted to the right. He wears a light-colored military uniform with an eagle badge on the left chest. His epaulettes are dark and have a light bar and star. He has a white shirt and a dark necktie.
Ensign Neil Armstrong on May 23, 1952
Armstrong's call-up from the Navy arrived on January 26, 1949, requiring him to report to Naval Air Station Pensacola in Florida for flight training with class 5-49. After passing the medical examinations, he became a midshipman on February 24, 1949.[17] Flight training was conducted in a North American SNJ trainer, in which he soloed on September 9, 1949.[18] On March 2, 1950, he made his first aircraft carrier landing on USS Cabot, an achievement he considered comparable to his first solo flight.[18] He was then sent to Naval Air Station Corpus Christi in Texas for training on the Grumman F8F Bearcat, culminating in a carrier landing on USS Wright. On August 16, 1950, Armstrong was informed by letter that he was a fully qualified naval aviator. His mother and sister attended his graduation ceremony on August 23, 1950.[19]

Armstrong was assigned to Fleet Aircraft Service Squadron 7 (FASRON 7) at NAS San Diego (now known as NAS North Island). On November 27, 1950, he was assigned to VF-51, an all-jet squadron, becoming its youngest officer, and made his first flight in a jet, a Grumman F9F Panther, on January 5, 1951. He was promoted to ensign on June 5, 1951, and made his first jet carrier landing on USS Essex two days later. On June 28, 1951, Essex had set sail for Korea, with VF-51 aboard to act as ground-attack aircraft. VF-51 flew ahead to Naval Air Station Barbers Point in Hawaii, where it conducted fighter-bomber training before rejoining the ship at the end of July.[20]

On August 29, 1951, Armstrong saw action in the Korean War as an escort for a photo reconnaissance plane over Songjin.[21] Five days later, on September 3, he flew armed reconnaissance over the primary transportation and storage facilities south of the village of Majon-ni, west of Wonsan. According to Armstrong, he was making a low bombing run at 350 mph (560 km/h) when 6 feet (1.8 m) of his wing was torn off after it collided with a cable that was strung across the hills as a booby trap. He was flying 500 feet (150 m) above the ground when he hit it. While there was heavy anti-aircraft fire in the area, none hit Armstrong's aircraft.[22] An initial report to the commanding officer of Essex said that Armstrong's F9F Panther was hit by anti-aircraft fire. The report indicated he was trying to regain control and collided with a pole, which sliced off 2 feet (0.61 m) of the Panther's right wing. Further perversions of the story by different authors added that he was only 20 feet (6.1 m) from the ground and that 3 feet (0.91 m) of his wing was sheared off.[23]

Two dark-blue-painted single-seat military jets flying from left to right in echelon. They wear the mark of the U.S. military on the nose, and a number. The nearer plane is 107 and the further is 116. On the fin is the letter 'S' and just in front the word NAVY. The planes have wingtip drop tanks and bubble canopies.
F9F-2 Panthers over Korea, with Armstrong piloting S-116 (left)
Armstrong flew the plane back to friendly territory, but due to the loss of the aileron, ejection was his only safe option. He intended to eject over water and await rescue by Navy helicopters, but his parachute was blown back over land. A jeep driven by a roommate from flight school picked him up; it is unknown what happened to the wreckage of his aircraft, F9F-2 BuNo 125122.[24]

In all, Armstrong flew 78 missions over Korea for a total of 121 hours in the air, a third of them in January 1952, with the final mission on March 5, 1952. Of 492 U.S. Navy personnel killed in the Korean War, 27 of them were from Essex on this war cruise. Armstrong received the Air Medal for 20 combat missions, two gold stars for the next 40, the Korean Service Medal and Engagement Star, the National Defense Service Medal, and the United Nations Korea Medal.[25]

Armstrong's regular commission was terminated on February 25, 1952, and he became an ensign in the United States Navy Reserve. On completion of his combat tour with Essex, he was assigned to a transport squadron, VR-32, in May 1952. He was released from active duty on August 23, 1952, but remained in the reserve, and was promoted to lieutenant (junior grade) on May 9, 1953.[26] As a reservist, he continued to fly, with VF-724 at Naval Air Station Glenview in Illinois, and then, after moving to California, with VF-773 at Naval Air Station Los Alamitos.[27] He remained in the reserve for eight years, before resigning his commission on October 21, 1960.[26]

College years
After his service with the Navy, Armstrong returned to Purdue. His previously earned good but not outstanding grades now improved, lifting his final Grade Point Average (GPA) to a respectable but not outstanding 4.8 out of 6.0. He pledged the Phi Delta Theta fraternity, and lived in its fraternity house. He wrote and co-directed two musicals as part of the all-student revue. The first was a version of Snow White and the Seven Dwarfs, co-directed with his girlfriend Joanne Alford from the Alpha Chi Omega sorority, with songs from the Walt Disney film, including "Someday My Prince Will Come"; the second was titled The Land of Egelloc ("college" spelled backwards), with music from Gilbert and Sullivan but new lyrics. He was chairman of the Purdue Aero Flying Club, and flew the club's aircraft, an Aeronca and a couple of Pipers, which were kept at nearby Aretz Airport in Lafayette, Indiana. Flying the Aeronca to Wapakoneta in 1954, he damaged it in a rough landing in a farmer's field, and it had to be hauled back to Lafayette on a trailer.[28] He was a baritone player in the Purdue All-American Marching Band.[29] Ten years later he was made an honorary member of Kappa Kappa Psi national band honorary fraternity.[30] Armstrong graduated with a Bachelor of Science degree in Aeronautical Engineering in January 1955.[27] In 1970, he completed his Master of Science degree in Aerospace Engineering at the University of Southern California (USC).[31] He would eventually be awarded honorary doctorates by several universities.[32]

Armstrong met Janet Elizabeth Shearon, who was majoring in home economics, at a party hosted by Alpha Chi Omega.[33] According to the couple, there was no real courtship, and neither could remember the exact circumstances of their engagement. They were married on January 28, 1956, at the Congregational Church in Wilmette, Illinois. When he moved to Edwards Air Force Base, he lived in the bachelor quarters of the base, while Janet lived in the Westwood district of Los Angeles. After one semester, they moved into a house in Antelope Valley, near Edwards AFB. Janet did not finish her degree, a fact she regretted later in life. The couple had three children: Eric, Karen, and Mark.[34] In June 1961, Karen was diagnosed with a diffuse intrinsic pontine glioma, a malignant tumor of the middle part of her brain stem.[35] X-ray treatment slowed its growth, but her health deteriorated to the point where she could no longer walk or talk. She died of pneumonia, related to her weakened health, on January 28, 1962, aged two.[36]

Test pilot
Following his graduation from Purdue, Armstrong became an experimental research test pilot. He applied at the National Advisory Committee for Aeronautics (NACA) High-Speed Flight Station at Edwards Air Force Base.[37] NACA had no open positions, and forwarded his application to the Lewis Flight Propulsion Laboratory in Cleveland, where Armstrong made his first test flight on March 1, 1955.[37] Armstrong's stint at Cleveland lasted only a couple of months before a position at the High-Speed Flight Station became available, and he reported for work there on July 11, 1955.[38]

A black-and-white photo of a young man with light skin and pale irises. His mid-colored hair is cut short. He is looking at the camera. He is wearing a barleycorn sport coat, a white shirt and a dark necktie.
Armstrong, 26, as a test pilot at the NACA High-Speed Flight Station at Edwards AFB, California
On his first day, Armstrong was tasked with piloting chase planes during releases of experimental aircraft from modified bombers. He also flew the modified bombers, and on one of these missions had his first flight incident at Edwards. On March 22, 1956, he was in a Boeing B-29 Superfortress,[39] which was to air-drop a Douglas D-558-2 Skyrocket. He sat in the right-hand pilot seat while the left-hand seat commander, Stan Butchart, flew the B-29.[40]

As they climbed to 30,000 feet (9 km), the number-four engine stopped and the propeller began windmilling (rotating freely) in the airstream. Hitting the switch that would stop the propeller's spinning, Butchart found it slowed but then started spinning again, this time even faster than the others; if it spun too fast, it would break apart. Their aircraft needed to hold an airspeed of 210 mph (338 km/h) to launch its Skyrocket payload, and the B-29 could not land with the Skyrocket attached to its belly. Armstrong and Butchart brought the aircraft into a nose-down attitude to increase speed, then launched the Skyrocket. At the instant of launch, the number-four engine propeller disintegrated. Pieces of it damaged the number-three engine and hit the number-two engine. Butchart and Armstrong were forced to shut down the damaged number-three engine, along with the number-one engine, due to the torque it created. They made a slow, circling descent from 30,000 ft (9 km) using only the number-two engine, and landed safely.[41]

Armstrong served as project pilot on Century Series fighters, including the North American F-100 Super Sabre A and C variants, the McDonnell F-101 Voodoo, the Lockheed F-104 Starfighter, the Republic F-105 Thunderchief and the Convair F-106 Delta Dart. He also flew the Douglas DC-3, Lockheed T-33 Shooting Star, North American F-86 Sabre, McDonnell Douglas F-4 Phantom II, Douglas F5D-1 Skylancer, Boeing B-29 Superfortress, Boeing B-47 Stratojet and Boeing KC-135 Stratotanker, and was one of eight elite pilots involved in the Parasev paraglider research vehicle program.[42] Over his career, he flew more than 200 different models of aircraft.[31] His first flight in a rocket-powered aircraft was on August 15, 1957, in the Bell X-1B, to an altitude of 11.4 miles (18.3 km). On landing, the poorly designed nose landing gear failed, as had happened on about a dozen previous flights of the Bell X-1B. He flew the North American X-15 seven times,[43] including the first flight with the Q-ball system, the first flight of the number 3 X-15 airframe, and the first flight of the MH-96 adaptive flight control system.[44][45] He became an employee of the National Aeronautics and Space Administration (NASA) when it was established on October 1, 1958, absorbing NACA.[46]

Armstrong was involved in several incidents that went down in Edwards folklore or were chronicled in the memoirs of colleagues. During his sixth X-15 flight on April 20, 1962, Armstrong was testing the MH-96 control system when he flew to a height of over 207,000 feet (63 km) (the highest he flew before Gemini 8). He held up the aircraft nose for too long during its descent to demonstrate the MH-96's g-limiting performance, and the X-15 ballooned back up to around 140,000 feet (43 km). He flew past the landing field at Mach 3 at over 100,000 feet (30 km) in altitude, and ended up 40 miles (64 km) south of Edwards. After sufficient descent, he turned back toward the landing area, and landed, just missing Joshua trees at the south end. It was the longest X-15 flight in both flight time and length of the ground track.[47]

A black-and-white photo of Armstrong, with very short hair. He is smiling and is wearing a pressure suit and tall lace-up boots. Under his left arm he holds a bulky pressure helmet. He has black gloves on, and his right-hand rests on the nose of a dark-painted X-15 rocket plane with its canopy open. Armstrong and the plane are standing on a desert crust, and the plane's skids have left tracks in it.
Armstrong and X-15-1 after a research flight in 1960
Fellow astronaut Michael Collins wrote that of the X-15 pilots Armstrong "had been considered one of the weaker stick-and-rudder men, but the very best when it came to understanding the machine's design and how it operated".[48] Many of the test pilots at Edwards praised Armstrong's engineering ability. Milt Thompson said he was "the most technically capable of the early X-15 pilots". Bill Dana said Armstrong "had a mind that absorbed things like a sponge". Those who flew for the Air Force tended to have a different opinion, especially people like Chuck Yeager and Pete Knight, who did not have engineering degrees. Knight said that pilot-engineers flew in a way that was "more mechanical than it is flying", and gave this as the reason why some pilot-engineers got into trouble: Their flying skills did not come naturally.[49] Armstrong made seven flights in the X-15 between November 30, 1960, and July 26, 1962.[50] He reached a top speed of Mach 5.74 (3,989 mph, 6,420 km/h) in the X-15-1, and left the Flight Research Center with a total of 2,400 flying hours.[51]

On April 24, 1962, Armstrong flew for the only time with Yeager. Their job, flying a T-33, was to evaluate Smith Ranch Dry Lake in Nevada for use as an emergency landing site for the X-15. In his autobiography, Yeager wrote that he knew the lake bed was unsuitable for landings after recent rains, but Armstrong insisted on flying out anyway. As they attempted a touch-and-go, the wheels became stuck and they had to wait for rescue. As Armstrong told the story, Yeager never tried to talk him out of it and they made a first successful landing on the east side of the lake. Then Yeager told him to try again, this time a bit slower. On the second landing, they became stuck, provoking Yeager to fits of laughter.[52]

On May 21, 1962, Armstrong was involved in the "Nellis Affair". He was sent in an F-104 to inspect Delamar Dry Lake in southern Nevada, again for emergency landings. He misjudged his altitude and did not realize that the landing gear had not fully extended. As he touched down, the landing gear began to retract; Armstrong applied full power to abort the landing, but the ventral fin and landing gear door struck the ground, damaging the radio and releasing hydraulic fluid. Without radio communication, Armstrong flew south to Nellis Air Force Base, past the control tower, and waggled his wings, the signal for a no-radio approach. The loss of hydraulic fluid caused the tailhook to release, and upon landing, he caught the arresting wire attached to an anchor chain, and dragged the chain along the runway.[53]

It took thirty minutes to clear the runway and rig another arresting cable. Armstrong telephoned Edwards and asked for someone to collect him. Milt Thompson was sent in an F-104B, the only two-seater available, but a plane Thompson had never flown. With great difficulty, Thompson made it to Nellis, where a strong crosswind caused a hard landing and the left main tire suffered a blowout. The runway was again closed to clear it, and Bill Dana was sent to Nellis in a T-33, but he almost landed long. The Nellis base operations office then decided that to avoid any further problems, it would be best to find the three NASA pilots ground transport back to Edwards.[53]

Astronaut career
Armstrong standing up, wearing an early space suit. It is highly reflective silver in appearance. He is wearing the helmet, which is white, with the visor raised. A thick dark hose is connected to one of the two ports on the front abdomen of the suit.
Armstrong in an early Gemini space suit
In June 1958, Armstrong was selected for the U.S. Air Force's Man In Space Soonest program, but the Advanced Research Projects Agency (ARPA) canceled its funding on August 1, 1958, and on November 5, 1958, it was superseded by Project Mercury, a civilian project run by NASA. As a NASA civilian test pilot, Armstrong was ineligible to become one of its astronauts at this time, as selection was restricted to military test pilots.[54][55] In November 1960, he was chosen as part of the pilot consultant group for the X-20 Dyna-Soar, a military space plane under development by Boeing for the U.S. Air Force, and on March 15, 1962, he was selected by the U.S. Air Force as one of seven pilot-engineers who would fly the X-20 when it got off the design board.[56][57]

In April 1962, NASA sought applications for the second group of NASA astronauts for Project Gemini, a proposed two-man spacecraft. This time, selection was open to qualified civilian test pilots.[58] Armstrong visited the Seattle World's Fair in May 1962 and attended a conference there on space exploration that was co-sponsored by NASA. After he returned from Seattle on June 4, he applied to become an astronaut. His application arrived about a week past the June 1, 1962, deadline, but Dick Day, a flight simulator expert with whom Armstrong had worked closely at Edwards, saw the late arrival of the application and slipped it into the pile before anyone noticed.[59] At Brooks Air Force Base at the end of June, Armstrong underwent a medical exam that many of the applicants described as painful and at times seemingly pointless.[60]

NASA's Director of Flight Crew Operations, Deke Slayton, called Armstrong on September 13, 1962, and asked whether he would be interested in joining the NASA Astronaut Corps as part of what the press dubbed "the New Nine"; without hesitation, Armstrong said yes. The selections were kept secret until three days later, although newspaper reports had circulated since earlier that year that he would be selected as the "first civilian astronaut".[61] Armstrong was one of two civilian pilots selected for this group;[62] the other was Elliot See, another former naval aviator.[63] NASA selected the second group that, compared with the Mercury Seven astronauts, were younger,[60] and had more impressive academic credentials.[64] Collins wrote that Armstrong was by far the most experienced test pilot in the Astronaut Corps.[48]

Gemini program
Gemini 5
On February 8, 1965, Armstrong and Elliot See were picked as the backup crew for Gemini 5, with Armstrong as commander, supporting the prime crew of Gordon Cooper and Pete Conrad.[65] The mission's purpose was to practice space rendezvous and to develop procedures and equipment for a seven-day flight, all of which would be required for a mission to the Moon. With two other flights (Gemini 3 and Gemini 4) in preparation, six crews were competing for simulator time, so Gemini 5 was postponed. It finally lifted off on August 21.[66] Armstrong and See watched the launch at Cape Kennedy, then flew to the Manned Spacecraft Center (MSC) in Houston.[67] The mission was generally successful, despite a problem with the fuel cells that prevented a rendezvous. Cooper and Conrad practiced a "phantom rendezvous", carrying out the maneuver without a target.[68]

Gemini 8
Main article: Gemini 8
Armstrong, with short hair, partially reclining on a beige chair. He looks very serious. He is wearing a white space suit without a helmet or gloves. It has the U.S. flag on the left shoulder. Two hoses are attached. A technician dressed all in white is bending over him. A dark-haired, darkly dressed man has his back to us. He may be talking to Armstrong.
Armstrong, 35, suiting up for Gemini 8 in March 1966
The crews for Gemini 8 were assigned on September 20, 1965. Under the normal rotation system, the backup crew for one mission became the prime crew for the third mission after, but Slayton designated David Scott as the pilot of Gemini 8.[69][70] Scott was the first member of the third group of astronauts, who was selected on October 18, 1963, to receive a prime crew assignment.[71] See was designated to command Gemini 9. Henceforth, each Gemini mission was commanded by a member of Armstrong's group, with a member of Scott's group as the pilot. Conrad would be Armstrong's backup this time, and Richard F. Gordon Jr. his pilot.[69][70] Armstrong became the first American civilian in space. (Valentina Tereshkova of the Soviet Union had become the first civilian—and first woman—nearly three years earlier aboard Vostok 6 when it launched on June 16, 1963.[72]) Armstrong would also be the last of his group to fly in space, as See died in a T-38 crash on February 28, 1966, that also took the life of crewmate Charles Bassett. They were replaced by the backup crew of Tom Stafford and Gene Cernan, while Jim Lovell and Buzz Aldrin moved up from the backup crew of Gemini 10 to become the backup for Gemini 9,[73] and would eventually fly Gemini 12.[74]

Gemini 8 launched on March 16, 1966. It was the most complex mission yet, with a rendezvous and docking with an uncrewed Agena target vehicle, and the planned second American space walk (EVA) by Scott. The mission was planned to last 75 hours and 55 orbits. After the Agena lifted off at 10:00:00 EST,[75] the Titan II rocket carrying Armstrong and Scott ignited at 11:41:02 EST, putting them into an orbit from which they chased the Agena.[76] They achieved the first-ever docking between two spacecraft.[77] Contact with the crew was intermittent due to the lack of tracking stations covering their entire orbits. While out of contact with the ground, the docked spacecraft began to roll, and Armstrong attempted to correct this with the Gemini's Orbit Attitude and Maneuvering System (OAMS). Following the earlier advice of Mission Control, they undocked, but the roll increased dramatically until they were turning about once per second, indicating a problem with Gemini's attitude control. Armstrong engaged the Reentry Control System (RCS) and turned off the OAMS. Mission rules dictated that once this system was turned on, the spacecraft had to reenter at the next possible opportunity. It was later thought that damaged wiring caused one of the thrusters to stick in the on position.[78]

A dark gray Gemini capsule floats horizontally in blue water. It is supported by a yellow flotation collar. The hatches are open and the astronauts are visible sitting in their places wearing sunglasses. They are being assisted by three recovery crew in dark gray wetsuits.
Recovery of Gemini 8 from the western Pacific Ocean; Armstrong sitting to the right
A few people in the Astronaut Office, including Walter Cunningham, felt that Armstrong and Scott "had botched their first mission".[79] There was speculation that Armstrong could have salvaged the mission if he had turned on only one of the two RCS rings, saving the other for mission objectives. These criticisms were unfounded; no malfunction procedures had been written, and it was possible to turn on only both RCS rings, not one or the other.[80] Gene Kranz wrote, "The crew reacted as they were trained, and they reacted wrong because we trained them wrong." The mission planners and controllers had failed to realize that when two spacecraft were docked, they must be considered one spacecraft. Kranz considered this the mission's most important lesson.[81] Armstrong was depressed that the mission was cut short,[82] canceling most mission objectives and robbing Scott of his EVA. The Agena was later reused as a docking target by Gemini 10.[83] Armstrong and Scott received the NASA Exceptional Service Medal,[84][85] and the Air Force awarded Scott the Distinguished Flying Cross as well.[86] Scott was promoted to lieutenant colonel, and Armstrong received a $678 raise in pay to $21,653 a year (equivalent to $172,713 in 2020), making him NASA's highest-paid astronaut.[82]

Gemini 11
Main article: Gemini 11
In Armstrong's final assignment in the Gemini program, he was the back-up Command Pilot for Gemini 11. Having trained for two flights, Armstrong was quite knowledgeable about the systems and took on a teaching role for the rookie backup Pilot, William Anders.[87] The launch was on September 12, 1966,[88] with Conrad and Gordon on board, who successfully completed the mission objectives, while Armstrong served as a capsule communicator (CAPCOM).[89]

Following the flight, President Lyndon B. Johnson asked Armstrong and his wife to take part in a 24-day goodwill tour of South America.[90] Also on the tour, which took in 11 countries and 14 major cities, were Dick Gordon, George Low, their wives, and other government officials. In Paraguay, Armstrong greeted dignitaries in their local language, Guarani; in Brazil he talked about the exploits of the Brazilian-born aviation pioneer Alberto Santos-Dumont.[91]

Apollo program
On January 27, 1967—the day of the Apollo 1 fire—Armstrong was in Washington, D.C. with Cooper, Gordon, Lovell and Scott Carpenter for the signing of the United Nations Outer Space Treaty. The astronauts chatted with the assembled dignitaries until 18:45, when Carpenter went to the airport, and the others returned to the Georgetown Inn, where they each found messages to phone the MSC. During these calls, they learned of the deaths of Gus Grissom, Ed White and Roger Chaffee in the fire. Armstrong and the group spent the rest of the night drinking scotch and discussing what had happened.[92]

On April 5, 1967, the same day the Apollo 1 investigation released its final report, Armstrong and 17 other astronauts gathered for a meeting with Slayton. The first thing Slayton said was, "The guys who are going to fly the first lunar missions are the guys in this room."[93] According to Cernan, only Armstrong showed no reaction to the statement. To Armstrong it came as no surprise—the room was full of veterans of Project Gemini, the only people who could fly the lunar missions. Slayton talked about the planned missions and named Armstrong to the backup crew for Apollo 9, which at that stage was planned as a medium Earth orbit test of the combined lunar module and command and service module.[94]

The crew was officially assigned on November 20, 1967.[95] For crewmates, Armstrong was assigned Lovell and Aldrin, from Gemini 12. After design and manufacturing delays of the lunar module (LM), Apollo 8 and 9 swapped prime and backup crews. Based on the normal crew rotation, Armstrong would command Apollo 11,[94] with one change: Collins on the Apollo 8 crew began experiencing trouble with his legs. Doctors diagnosed the problem as a bony growth between his fifth and sixth vertebrae, requiring surgery.[96] Lovell took his place on the Apollo 8 crew, and, when Collins recovered, he joined Armstrong's crew.[97]

An indistinct photo of a smoke trail rising from an area of orange fire in a recently harvested field. A white and orange parachute is recovering a human figure above and to the right of the fire.
Armstrong descends to the ground on a parachute after ejecting from Lunar Landing Research Vehicle 1.
To give the astronauts practice piloting the LM on its descent, NASA commissioned Bell Aircraft to build two Lunar Landing Research Vehicles (LLRV), later augmented with three Lunar Landing Training Vehicles (LLTV). Nicknamed the "Flying Bedsteads", they simulated the Moon's one-sixth gravity using a turbofan engine to support five-sixths of the craft's weight. On May 6, 1968, 100 feet (30 m) above the ground, Armstrong's controls started to degrade and the LLRV began rolling.[98] He ejected safely before the vehicle struck the ground and burst into flames. Later analysis suggested that if he had ejected half a second later, his parachute would not have opened in time. His only injury was from biting his tongue. The LLRV was completely destroyed.[99] Even though he was nearly killed, Armstrong maintained that without the LLRV and LLTV, the lunar landings would not have been successful, as they gave commanders essential experience in piloting the lunar landing craft.[100]

In addition to the LLRV training, NASA began lunar landing simulator training after Apollo 10 was completed. Aldrin and Armstrong trained for a variety of scenarios that could develop during a real lunar landing.[101] They also received briefings from geologists at NASA.[102]

Apollo 11
Main article: Apollo 11
Three astronauts in white space suits. They are holding their helmets. All are light-skinned. Armstrong is smiling widely and wears his hair parted to the right. Collins has dark hair and looks the most serious. Aldrin's hair is very short. Behind them is a large photo of the Moon.
The Apollo 11 crew: Armstrong, Michael Collins, and Buzz Aldrin.
After Armstrong served as backup commander for Apollo 8, Slayton offered him the post of commander of Apollo 11 on December 23, 1968, as Apollo 8 orbited the Moon.[103] According to Armstrong's 2005 biography, Slayton told him that although the planned crew was Commander Armstrong, Lunar Module Pilot Buzz Aldrin, and Command Module Pilot Michael Collins, he was offering Armstrong the chance to replace Aldrin with Jim Lovell. After thinking it over for a day, Armstrong told Slayton he would stick with Aldrin, as he had no difficulty working with him and thought Lovell deserved his own command. Replacing Aldrin with Lovell would have made Lovell the lunar module pilot, unofficially the lowest ranked member, and Armstrong could not justify placing Lovell, the commander of Gemini 12, in the number 3 position of the crew.[104] The crew of Apollo 11 was assigned on January 9, 1969, as Armstrong, Collins, and Aldrin, with Lovell, Anders, and Fred Haise as the backup crew.[105]

According to Chris Kraft, a March 1969 meeting among Slayton, George Low, Bob Gilruth, and Kraft determined that Armstrong would be the first person on the Moon, in part because NASA management saw him as a person who did not have a large ego. A press conference on April 14, 1969, gave the design of the LM cabin as the reason for Armstrong's being first; the hatch opened inwards and to the right, making it difficult for the LM pilot, on the right-hand side, to exit first. At the time of their meeting, the four men did not know about the hatch consideration. The first knowledge of the meeting outside the small group came when Kraft wrote his book.[106][107] Methods of circumventing this difficulty existed, but it is not known if these were considered at the time. Slayton added, "Secondly, just on a pure protocol basis, I figured the commander ought to be the first guy out ... I changed it as soon as I found they had the time line that showed that. Bob Gilruth approved my decision."[108]

Voyage to the Moon
A Saturn V rocket launched Apollo 11 from Launch Complex 39A at the Kennedy Space Center on July 16, 1969, at 13:32:00 UTC (09:32:00 EDT local time).[109] Armstrong's wife Janet and two sons watched from a yacht moored on the Banana River.[110] During the launch, Armstrong's heart rate peaked at 110 beats per minute.[111] He found the first stage the loudest, much noisier than the Gemini 8 Titan II launch. The Apollo command module was relatively roomy compared with the Gemini spacecraft. None of the Apollo 11 crew suffered space sickness, as some members of previous crews had. Armstrong was especially glad about this, as he had been prone to motion sickness as a child and could experience nausea after long periods of aerobatics.[112]

Armstrong smiling in his space suit with the helmet off. He wears a headset and his eyes look slightly watery.
Armstrong in the lunar module after the completion of the EVA
Apollo 11's objective was to land safely on the Moon, rather than to touch down at a precise location. Three minutes into the lunar descent, Armstrong noted that craters were passing about two seconds too early, which meant the Lunar Module Eagle would probably touch down several miles (kilometres) beyond the planned landing zone.[113] As the Eagle's landing radar acquired the surface, several computer error alarms sounded. The first was a code 1202 alarm, and even with their extensive training, neither Armstrong nor Aldrin knew what this code meant. They promptly received word from CAPCOM Charles Duke in Houston that the alarms were not a concern; the 1202 and 1201 alarms were caused by executive overflows in the lunar module guidance computer. In 2007, Aldrin said the overflows were caused by his own counter-checklist choice of leaving the docking radar on during the landing process, causing the computer to process unnecessary radar data. When it did not have enough time to execute all tasks, the computer dropped the lower-priority ones, triggering the alarms. Aldrin said he decided to leave the radar on in case an abort was necessary when re-docking with the Apollo command module; he did not realize it would cause the processing overflows.[114]

File:AP11 FINAL APPROACH.ogv
Armstrong lands the Lunar Module Eagle on the Moon, July 20, 1969
When Armstrong noticed they were heading toward a landing area that seemed unsafe, he took manual control of the LM and attempted to find a safer area. This took longer than expected, and longer than most simulations had taken.[115] For this reason, Mission Control was concerned that the LM was running low on fuel.[116] On landing, Aldrin and Armstrong believed they had 40 seconds of fuel left, including the 20 seconds' worth which had to be saved in the event of an abort.[117] During training, Armstrong had, on several occasions, landed with fewer than 15 seconds of fuel; he was also confident the LM could survive a fall of up to 50 feet (15 m). Post-mission analysis showed that at touchdown there were 45 to 50 seconds of propellant burn time left.[118]

The landing on the surface of the Moon occurred several seconds after 20:17:40 UTC on July 20, 1969.[119] One of three 67-inch (170 cm) probes attached to three of the LM's four legs made contact with the surface, a panel light in the LM illuminated, and Aldrin called out, "Contact light." Armstrong shut the engine off and said, "Shutdown." As the LM settled onto the surface, Aldrin said, "Okay, engine stop"; then they both called out some post-landing checklist items. After a 10-second pause, Duke acknowledged the landing with, "We copy you down, Eagle." Armstrong confirmed the landing to Mission Control and the world with the words, "Houston, Tranquility Base here. The Eagle has landed." Aldrin and Armstrong celebrated with a brisk handshake and pat on the back. They then returned to the checklist of contingency tasks, should an emergency liftoff become necessary.[120][121][122] After Armstrong confirmed touch down, Duke re-acknowledged, adding a comment about the flight crew's relief: "Roger, Tranquility. We copy you on the ground. You got a bunch of guys about to turn blue. We're breathing again. Thanks a lot."[117] During the landing, Armstrong's heart rate ranged from 100 to 150 beats per minute.[123]

First Moon walk
See also: Apollo 11—Lunar surface operations
File:Apollo 11 Landing - first steps on the moon.ogv
Armstrong describes the lunar surface

"That's one small step for man, one giant leap for mankind" (0:08)
MENU0:00
Problems playing this file? See media help.
The flight plan called for a crew rest period before leaving the module, but Armstrong asked for this be moved to earlier in the evening, Houston time. When he and Aldrin were ready to go outside, Eagle was depressurized, the hatch was opened, and Armstrong made his way down the ladder.[124] At the bottom of the ladder Armstrong said, "I'm going to step off the LM [lunar module] now". He turned and set his left boot on the lunar surface at 02:56 UTC July 21, 1969,[125] then said, "That's one small step for [a] man, one giant leap for mankind."[126] The exact timing of Armstrong's first step on the Moon is unclear.[127]

Armstrong prepared his famous epigram on his own.[128] In a post-flight press conference, he said that he chose the words "just prior to leaving the LM."[129] In a 1983 interview in Esquire magazine, he explained to George Plimpton: "I always knew there was a good chance of being able to return to Earth, but I thought the chances of a successful touch down on the moon surface were about even money—fifty–fifty ... Most people don't realize how difficult the mission was. So it didn't seem to me there was much point in thinking of something to say if we'd have to abort landing."[128] In 2012, his brother Dean Armstrong said that Neil showed him a draft of the line months before the launch.[130] Historian Andrew Chaikin, who interviewed Armstrong in 1988 for his book A Man on the Moon, disputed that Armstrong claimed to have conceived the line during the mission.[131]

Recordings of Armstrong's transmission do not provide evidence for the indefinite article "a" before "man", though NASA and Armstrong insisted for years that static obscured it. Armstrong stated he would never make such a mistake, but after repeated listenings to recordings, he eventually conceded he must have dropped the "a".[126] He later said he "would hope that history would grant me leeway for dropping the syllable and understand that it was certainly intended, even if it was not said—although it might actually have been".[132] There have since been claims and counter-claims about whether acoustic analysis of the recording reveals the presence of the missing "a";[126][133] Peter Shann Ford, an Australian computer programmer, conducted a digital audio analysis and claims that Armstrong did say "a man", but the "a" was inaudible due to the limitations of communications technology of the time.[126][134][135] Ford and James R. Hansen, Armstrong's authorized biographer, presented these findings to Armstrong and NASA representatives, who conducted their own analysis.[136] Armstrong found Ford's analysis "persuasive."[137][138] Linguists David Beaver and Mark Liberman wrote of their skepticism of Ford's claims on the blog Language Log.[139] A 2016 peer-reviewed study again concluded Armstrong had included the article.[140] NASA's transcript continues to show the "a" in parentheses.[141]

When Armstrong made his proclamation, Voice of America was rebroadcast live by the BBC and many other stations worldwide. An estimated 530 million people viewed the event,[142] 20 percent out of a world population of approximately 3.6 billion.[143][144] 

Q: Did you misspeak?

A: There isn't any way of knowing.

Q: Several sources say you did.

A: I mean, there isn't any way of my knowing. When I listen to the tape, I can't hear the 'a', but that doesn't mean it wasn't there, because that was the fastest VOX ever built. There was no mike-switch — it was a voice-operated key or VOX. In a helmet you find you lose a lot of syllables. Sometimes a short syllable like 'a' might not be transmitted. However, when I listen to it, I can't hear it. But the 'a' is implied, so I'm happy if they just put it in parentheses.

Omni, June 1982, p. 126
A grainy picture from behind of a human figure in white space suit and backpack standing in front of the Lunar Module on the surface of the Moon. A landing leg is visible and the U.S. flag on the descent stage.
Armstrong on the Moon
About 19 minutes after Armstrong's first step, Aldrin joined him on the surface, becoming the second human to walk on the Moon. They began their tasks of investigating how easily a person could operate on the lunar surface. Armstrong unveiled a plaque commemorating the flight, and with Aldrin, planted the flag of the United States. Although Armstrong had wanted the flag to be draped on the flagpole,[145] it was decided to use a metal rod to hold it horizontally.[146] However, the rod did not fully extend, leaving the flag with a slightly wavy appearance, as if there were a breeze.[147] Shortly after the flag planting, President Richard Nixon spoke to them by telephone from his office. He spoke for about a minute, after which Armstrong responded for about thirty seconds.[148] In the Apollo 11 photographic record, there are only five images of Armstrong partly shown or reflected. The mission was planned to the minute, with the majority of photographic tasks performed by Armstrong with the single Hasselblad camera.[149]

After helping to set up the Early Apollo Scientific Experiment Package, Armstrong went for a walk to what is now known as East Crater, 65 yards (59 m) east of the LM, the greatest distance traveled from the LM on the mission. His final task was to remind Aldrin to leave a small package of memorial items to Soviet cosmonauts Yuri Gagarin and Vladimir Komarov, and Apollo 1 astronauts Grissom, White and Chaffee.[150] The Apollo 11 EVA lasted two and a half hours.[151] Each of the subsequent five landings was allotted a progressively longer EVA period; the crew of Apollo 17 spent over 22 hours exploring the lunar surface.[151] In a 2010 interview, Armstrong explained that NASA limited their Moon walk because they were unsure how the space suits would cope with the Moon's extremely high temperature.[152]

Return to Earth
The three crew members smiling at the President through the glass window of their metal quarantine chamber. Below the window is the Presidential Seal, and above it is stenciled on a wooden board "HORNET + 3". President Nixon is standing at a microphone, also smiling. He has dark crinkly hair and a light gray suit.
The Apollo 11 crew and President Nixon during the post-mission quarantine period
After they re-entered the LM, the hatch was closed and sealed. While preparing for liftoff, Armstrong and Aldrin discovered that, in their bulky space suits, they had broken the ignition switch for the ascent engine; using part of a pen, they pushed in the circuit breaker to start the launch sequence.[153] The Eagle then continued to its rendezvous in lunar orbit, where it docked with Columbia, the command and service module. The three astronauts returned to Earth and splashed down in the Pacific Ocean, to be picked up by the USS Hornet.[154]

After being released from an 18-day quarantine to ensure that they had not picked up any infections or diseases from the Moon, the crew was feted across the United States and around the world as part of a 38-day "Giant Leap" tour.[155]


New York City ticker tape parade, August 13, 1969
The tour began on August 13, when the three astronauts spoke and rode in ticker-tape parades in their honor in New York and Chicago, with an estimated six million attendees.[156][157] On the same evening an official state dinner was held in Los Angeles to celebrate the flight, attended by members of Congress, 44 governors, the Chief Justice of the United States, and ambassadors from 83 nations. President Nixon and Vice President Agnew presented each astronaut with a Presidential Medal of Freedom.[156][158]

After the tour Armstrong took part in Bob Hope's 1969 Utail this year.SPEECHLESS LEARNING, MACHINE LEARNING EXPLANATIONS ARE SO EASYILY COVERED, EVEN A HISTORY PROFESSOR CAN USE IT. THANKING YOU FOR YOUR TIME AND CONSIDERATION. DR. RITESH PATEL GTU MBA SECTION HEAD GUJARAT TECHNOLOGICAL UNIVERSITY AHMEDABAD 9909944890 CUG PERSONAL 9687100199 [email protected]Thanks.Nice one,  but I need more explanation on unsupervised learning pleaseWhat questions do you have about unsupervised learning exactly?Examples of unsupervised machine learningThanks for the suggestion.Hi Jason,My problem is related to NLP and sentiment analysis.I have a dataset with a few columns. One of them is a free text and another one is a sentiment score, from 1 (negative) to 10 (positive).I’m trying to apply a sentiment analysis to the text field and see how well it works comparing with the sentiment score field. For this purpose, I’ve run some off-the-self sentiment analysis tools, such as Polyglot, but they didn’t work very well. That’s why I’ve decided to address this as a classification problem (negative, neutral or positive). In order to do this, I’ve got 1, 2 and 3-grams and I’ve used them as features to train my model. I tried with SVM and also getting the most representative grams for each of these classes using z-score, but the results were worst than with Polyglot.Any suggestion?Thanks!This tutorials will get you started:
https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/What are some widely used Python libraries for Supervised Learning?scikit-learn.What will be the best algorithm to use for a Prediction insurance claim  project?Try this process:
https://machinelearningmastery.com/start-here/#processHi Jason,you are awesome. Sorry if my question is meaningless. In simple what is relation between Big Data, Machine Learning, R, Python, Spark, Scala and Data Science?Thanks,
Sri.You can probably look up definitions of those terms. Why are you asking exactly?Hello Jason,
That was a good one, keep it up,
Please, what is your advised for a corporation that wants to use machine learning for archiving big data, developing AI that will help detect accurately similar interpretation and transform same into a software program.
Secondly, Beside these two areas, are there other areas you think AI will be helpful for industrialists. Let me know you take.ChibuzorI’m not sure how these methods could help with archiving.Perhaps this post will help you define your problem as a supervised learning problem:
http://machinelearningmastery.com/how-to-define-your-machine-learning-problem/Hi,
Interesting read.Do you have a suggestion for where for a given input (image) choosing a particular point p gives a reward r. the goal is to maximize r. There may me multiple points that return the same maximum r value, so I don’t see standard a cnn training methods working. It does not matter which one is returned the reward is the same. Each trial is separate so reinforcement learning does not seem correct.Sounds like a multimodal optimization problem. If you only need one result, one of a range of stochastic optimization algorithms can be used.If you need all points, then a multimodal optimizaiton could be used, like a niching genetic algorithm (I did my masters on these).Very Helping Material i was preparing for my exams and i have completely understood the whole concept it was very smoothly explained JAZAKALLA (Means May GOD give you HIS blessing )I’m glad it helped.you can give me  an explanation about the classes of unsupervised methods: by block, by pixel, by region which used in the segmentation.Sorry, I don’t follow. Perhaps you can provide more context?sir, can you tell real time example on supervised,unsupervised,semisupervisedLinear regression is supervised, clustering is unsupervised, autoencoders can be used in an semisupervised manner.Sir, thank u for such a great information.
But how can we use unsupervised learning for any type of clustering?Sorry, I don’t have material on clustering. I may cover it in the future.Thanks for posting this.  This is a great summary!  Very straightforward explanations.I’m glad it helped.First of all very nice and helpfull report, and then my question.I have an unsupervised dataset with people and i want to find some paterns about their behaviour for future marketing. I am using clustering algorythms but then if i want to train a model for future predictions (for a new entry in the dataset, or for a new transaction of an already registered person in the dataset) should i use these clusters as classes to train the model as supervised classification? Or how can i do this? i am confused. Thank you in advance!!Perhaps start with a clear idea of the outcomes you require and work backwards:
http://machinelearningmastery.com/how-to-define-your-machine-learning-problem/Thank you for your reply, but this couldnt help me too much.. Some people, after a clustering method in a unsupervised model ex. k-means use the k-means prediction to predict the cluster that a new entry belong. But some other after finding the clusters, train a new classifier ex. as the problem is now supervised with the clusters as classes, And use this classifier to predict the class or the cluster of the new entry. I cant understand the difference bettween these two methods. I dont know if you understand my point but i would appreciate if you try to explain it to me..Sorry, I don’t have material on clustering, I cannot give you good advice.Thank You for the giving better explanation.I’m glad it helped.given that some students information such as(Name,Address,GPA-1,GPA-2, and Grade),,,,my job is to “divide students based on their grade”…..so my question is the this job is supervise or unsupervised learning? and which Machine learning algorithm is perfect to do this job…I thing it will be Unsupervised learning but i am confused about what algorithm perfect for this job….(is it clustering)… am i right sir?This post might help you determine whether it is a supervised learning problem:
http://machinelearningmastery.com/how-to-define-your-machine-learning-problem/Hi Jason,I have documents with handwritten and machine printed texts. I want to localize the text in the document and find whether the text is handwritten or machine printed. If the text is handwritten, i have to give it to a handwritting recognition algorithm or if it is machine printed, I have to give it to tesseract ocr algorithm.Can you please suggest me how to do text localization and find whether the text is handwritten or machine printed..Thanks in advance,
HarathiI would recommend looking into computer vision methods. I do not cover this area sorry.hi, im new to machine learning im struck in the machine learning in training the data please help me with this, like Create a Keras neural network for anomaly detection,please can you fix the error i have tried several times no idea what is the problemstuck at task 3
check in gist url
features = train_both[:,:-1]
labels = train_both[:,:-1]ths gist url: https://gist.github.com/dcbeafda57395f1914d2aa5b62b08154I’m eager to help, but I don’t have the capacity to debug your code for you.Perhaps post on stackoverflow?Hi Jason, nice post btw.
I was wondering what’s the difference and advantage/disadvantage of different Neural Network supervised learning methods like Hebb Rule, Perceptron, Delta Rule, Backpropagation, etc and what problems are best used for each of them.We do not have a mapping of problems to algorithms in machine learning. The best we can do is empirically evaluate algorithms on a specific dataset to discover what works well/best.I need a brief description in machine learning and how it is applied. Where and when it were required?This might help:
https://machinelearningmastery.com/what-is-machine-learning/Amazing post.. Actual complete definitions are provided.. Thanks for it 🙂I’m glad it helped.Hi sir
Thank you advance for your article, it’s very nice and helpful
Iam new in machine learning and i would like to understand what is mean deep learning? Second, distance supervise wether like semisuperviser or not?Thanks advanceThis post explains more about deep learning:
https://machinelearningmastery.com/what-is-deep-learning/Hello Jason Brownlee,I was working on a health research project which would detect snore or not from input wav file. Can you please suggest which one i would prefer Supervised learning or Unsupervised learning or Semi-Supervised learning. i’m a iOS Developer and new to ML. Where do i start from?
Your advise will help a lot in my project.Thanks in Advance!Supervised.Start by defining the problem:
http://machinelearningmastery.com/how-to-define-your-machine-learning-problem/could you explain semi supervised machine learning a bit more with examples.Thanks for the suggestion. This might be a good place to start:
https://en.wikipedia.org/wiki/Semi-supervised_learningDear prof Brownlee:          From my understanding, method based on unsupervised leaning(no labels required) can’t compare with those based on supervised leaning(labels required) since their comparison premise is different. I f one wants to compare them, one should put them under the same problem scenarios,only this way, comparison is reasonable and fair,isn’i it? but provided that the problem scenarios are applictions without labels, they can’t compare with each other since supervised leaning methods need lables to train models,but now there are no labels to be trained, therefore I think it is unreasonable and infeasible to compare method based on unsupervised leaning with those based on supervised leaning,is it right? I want to know your views, thank you!Yes, they are not comparable. They solve different problems.Hi Jason,Your article was very informative and cleared lot of my concepts. I have lot of questions in my mind about Machine Learning. Is it possible you can guide me over Skype call and I am ready to pay.Why association rules are part of unsupervised learning?There is no training/teaching component, the rules are extracted from the data.Hello Jason,Great explanation,
i have a question , I am doing ML in JAVA ,can you suggest me how can i choose best algorithm for my data?
as i am using numeric data (Temperature sensor)  which method is best supervised or unsupervised ?
Hope u got my pointI recommend this framework:
https://machinelearningmastery.com/start-here/#processHello, I am Noel, I am new to machine learning with less experience. I want to make a machine learning model to predict the possibility of any attack or abnormal events/behavior to my system. the model should classify the situation based on the security level of it and give me the predictable cause and solution. What to do on this guysI recommend following this process for a new project:
https://machinelearningmastery.com/start-here/#processI’m thankful to you for such a nice article!
I would love to follow you and your articles further.Thanks.I never understood what the semi-supervised machine learning is, until I read your post. The issue was whether we can have new labels after processing or we are based only on the first given labels. The example you gave made it all clear. So, the answer is, we don’t have all the labels, that’s why we join unlabeled data. Thank you for your great posts!Thanks.Hey Jason,Love your books and articles. Any chance you’ll give us a tutorial on K-Means clustering in the near future?-KateThanks for the suggestion Kate.Hi Jason , Thanks for clarifying my dough’s between supervised and unsupervised machine learning. But one more dough’s , how can i justify or apply the correct algorithm  for particular problem . Is their any easy way to find out best algorithm for problem we get. Could you please let me know ?This is a common question that I answer here:
https://machinelearningmastery.com/faq/single-faq/what-algorithm-config-should-i-useThank you so much for all the time you put in for educating and replying to fellow learners. Thanks for being such an inspiration.Thanks, I’m just trying to be useful.which learning techniques could be better in particular machine learning domain?Good question, perhaps this will help:
https://machinelearningmastery.com/faq/single-faq/what-algorithm-config-should-i-useThe correct classes of training data are called supervisied r unsupervisedPredicting the class is a supervised problem.Hi Jason, thanks for this post.
I have a query regarding maximization of benefits and overcome the limitations from different types of regression algorithms in one system. Is it possible to create a data model such that I have ‘ONE’ data repository and 2 machine learning algorithms, say Logistic regression and Random Forest? The data repository is getting populated every minute (like in an information system) but after a span of 15 minutes, it is processed via Logistic Regression, and after the next 15 minutes, it is processed via Random Forest, and so on. My questions would be:
1. Is it possible to create such a system?
2. If yes, would this allow to gain benefits of both algorithms? If no, is there any alternative way to achieve this?Sure, I don’t see why not. The question is why would you want to do this?Well, I wanted to know if that can be regarded as an extension to ensemble modelling.I think some data critical applications, including IoT communication (let’s say, the domain of signal estimation for 5G, vehicle to vehicle communication) and information systems can make use of a cross check with multiple data models. In this way, the deficiencies of one model can be overcome by the other. Of course it would not be a memory/ hardware efficient solution, but just saying.If you have seen anything like this, a system where more than one data models are being used in one place, I would really appreciate you sharing it, thanks.In an ensemble, the output of two methods would be combined in some way in order to make a prediction.Hello, great job explaining all kind of MLA. but I am confused on where we can put the SVM in the Algorithms Mind Map?Thanks!Perhaps under instance based methods?I have learned up to machine learning algorithms,
now what is the next step to learn,i.e. which technology should i learn first
e.g. deep learning,opencv,NLP,neural network,or  image detection.
plz tell me step by step which one is interlinked and what should learn first.thanksPerhaps select a topic that most interests you or a topic that you can apply immediately:
https://machinelearningmastery.com/start-here/Hello,
I looked through your post because I have to use the Findex dataset from World Bank to get some information for my thesis on the factors influencing financial and digital inclusion of women. I’m thinking of using K-clustering for this project. I would like to get your input on this.Thank youIt really depends on the goals of your project.Perhaps this framework will help:
http://machinelearningmastery.com/how-to-define-your-machine-learning-problem/Hello JasonFirst of all thank you for the post. I’m working on a subject about identifying fake profiles on some social networks, the data that i have is unlabeled so i’m using unsupervised learning, but i need to do also a supervised learning. So my question is: can i label my data using the unsupervised learning at first so I can easily use it for supervised learning??Unsupervised learning can propose clusters, but you must still label data using an expert.Hi Jason, the information you provided was really helpful. I have a question, which machine learning algorithm is best suited for forensics investigation?This is a common question that I answer here:
https://machinelearningmastery.com/faq/single-faq/what-algorithm-config-should-i-useDear Jason,its been mentioned above that Supervised: ‘All data is labeled’.But its not mentioned that what does it mean that data is labeled or not?
if one get this kind of query while going through purchased e book, is there any support provided???
Note: For now I assume that labeled data mean for certain input X , output is /should be Y.Regards,
RandhirA label might be a class or it might be a target quantity.Hi  Jason,do you have any algorithm example for supervised learning and unsupervised learning?thank youI have many hundreds of examples, perhaps start here:
https://machinelearningmastery.com/start-here/Hi Jason, thanks for this great post. Do supervised methods use any unlabeled data at all? Or is the performance of the model evaluated on the basis of its classification (for categorical data) of the test data only? I am working on a project where I want to compare the performance of several supervised methods (SVMs, logistic regression, ensemble methods, random forests, and nearest neighbors) and one semi-supervised method (naive Bayes) in identifying a rare outcome, and I have about 2 million labeled records (split between training and test sets) and 200 million unlabeled records.Supervised learning models are evaluated on unseen data where we know the output.Thanks for this amazing post. I have read your many post.Thanks, I’m glad it helped.Thanks for the interested post, is great contribution on machine learning domain God bless youThanks.Hi Jason,
With unlabelled data, if we do kmeans and find the labels, now the data got labels, can we proceed to do supervised learning.
Thanksk-means will find clusters, not labels.Labels must be assigned by a domain expert.Thanks Jason, if they say there is going to be two clusters, then we build kmeans with K as 2, we get two clusters, in this case is this possible to continue supervised learning.Just to be more explainable, kmeansmodel = KMeans(n_clusters= 2)
kmeansmodel.fit(X_train)
predicted = kmeansmodel.labels_
kmf2labels = predicted.tolist()
raw_data[‘labels’] = kmf2labels Now we get labels as 0 and 1, so can we binary classification now.Yes.It may be.Thanks Jason, whether the supervised classification after unsupervised will improve our prediction results, may I have your comments please?It depends on the data and the model.The best that I can say is: try it and see.Hi Jason,
The DBSCAN model running into MemoryError(with 32GB RAM and 200,000 records, 60 Columns), may I know is there a solution for thisdbscan_model = DBSCAN(eps=3, min_samples=5, metric=’euclidean’, algorithm=’auto’)
dbscan_model.fit(X_scaled)I tried like splitting the data based on ONE categorical column, say Employed(Yes and No), so these two dataset splits getting 105,000 and 95000 records, so I build two models, for prediction if the test record is Employed Yes i run the model_Employed_Yes or other, NOT sure is this a good choice to do?
ThanksPerhaps try operating on a sample of the dataset?Perhaps try running on an EC2 instance with more memory?Perhaps try exploring a more memory efficient implementation?Hi
I used this note in my paper.
How can I reference it?
please help meGreat question, I show how here:
https://machinelearningmastery.com/faq/single-faq/how-do-i-reference-or-cite-a-book-or-blog-postThank you so much. I have one more question. Do we have the primal SVM function?
what is it?What is the “primal SVM function”? Do you mean the kernel?yes. the kernelPerhaps start here:
https://machinelearningmastery.com/support-vector-machines-for-machine-learning/thank you sir, this post is very helpful for me. sir i have a doubt. Is  unsupervised learning have dataset or not?Yes, unsupervised learning has a training dataset only.Training or testing?Yes.Hello sir. Thank you so much for this helping material.
Sir one problem i am facing that how can i identify the best suitable algorithm/model for a scenario.
For example i have an image and i want to find the values of three variables by ML model so which model can i use.Input: image
Output: concentration of variable 1, 2, 3 in an image.Perhaps try a range of CNN models for image classification?This might be a good place to start:
https://machinelearningmastery.com/start-here/#dlfcvi think the solution to unsupervised learning is to make a program that just takes photos from camera and then let the network reconstruct what ever total image that its confronted with by random and use this for method for its training. as far as i understand the network can reconstruct lots of images from fragments stored in the network. that means by take a snap shot of what camera sees and feed that as training data could pehaps solve unsupervised learning. this way the network automatically aquire it own training data. what i mean is not to classify data directly as that will keep you stuck in the supervised learning limbo.it will not be enough with one network. the reason is that it takes two players to share information. the network can’t read itself at the same time as it reconstruct as that obliterate the image its reconstructing from.what you need  is a second network that can reconstruct what the first network is showing. its not this simple either. you can not solve the problem by this alone as the network can only output a single image at the time so we need to break down the image into smaller parts and then let one network get a random piece to reconstruct the whole from the total image of the other networks reconstruction. by randomly trow the ball of part of the image between the networks, you have comunication between them. this way, you can make a dream like process with infinite possible images.
now you need a third network that can get random images received from the two other networks and use the input image data from the camera as images to compare the random suggestions from the two interchanging networks with the reconstruction from the third network from camera image. this way the machine will self classify the data that fits with the external image. what we need now is to brand these random images labels by marry the sound data or transelation of sound to speach with the random images from the two recursive mirrors secondary network to one primary by a algorithm that can take the repetition of recognized words done by another specialized network and indirectly use the condition for the recognition of the sound data as a trigger to take a snapshot of camera and reconstruct that image and then compare that image by the random recursive mirrors. if it found the image of the target in the camera in the random recursive network, you can then use a conventional algoritm to classify the recognized word with the recognized image. this way the machine will learn and teach itself information that over time will make it able to recall classified objects you did not teach it.this is not the solution of the whole problem. you do not have  Artificial General Intelligence yet. there is still a big problem left. you now have to find a way to make the software make comunication with people so that it can learn from their thinking and learn how to say things.what you have from before is just a very intelligent dream machine that learns.now we have to reverse the process. now we have to take input data from a person verbally and use the classifications the computer created by itself to reconstruct image in the main network. this way we are half way into letting the network learn from your verbal language by dive into its own network for information to create new and more classifications by itself using its previous methods. at this point you have created a very clever low iq program that only mirrors your saying like a evolved monkey.in order to solve this you have to increase the complexity of the networks by take the primary network and make it seconday and then create a new network that can act as the top of the triangle and make 6 seconday network that mimic the main network. these 6 networks will be handles to store parts of information that can make suggestions to compare to the main network output. this way you have 6 networks that contain pattern where they can compete for the better question or answer.  what ever it made  the program smarter i don’t know. byond this im clueless. anyway this is just an idea. if this is to complicated, there is no way in the world anyone will ever solve the problem of unsupervised learning that leads to agi.Thanks for sharing.Are supervised and unsupervised algorithms another way of defining parametric and nonparametric algorithms?No.Some supervised algorithms are parametric, some are nonparametric.Some unsupervised algorithms are parametric, some are nonparametric.great work,sir can you give example how supervised learning is used to test software components.
means how to do testing of software with supervised learning . any example will be helpfulThanks for the suggestion.Sir can you help me how to do testing with supervised learning. Please give any example. I am facing problem in itYes, there are hundreds of examples on the blog. Perhaps start here:
https://machinelearningmastery.com/machine-learning-in-python-step-by-step/You did a really good job with this.  I like it a lot. 🙂Thanks!ery informing article that tells differences between supervised and unsupervised learning!
thanks!You’re welcome!Hey Jason! Great article! I’m currently working on a Supervised/Unsupervised Learning Project for one of my MBA classes. For the project we have to identify a problem in our workplace that can be solved using Supervised and Unsupervised Learning. I work for a digital marketing agency that builds and manages marketing campaigns for small to mid size business (PPC, SEO, Facebook Ads, Display Ads, etc). For my unsupervised learning model I was thinking of solving the problem of customer churn before it gets to that point. I would use K-means Clustering and the features/columns for the model would be:– the reason for the cancellation
– how many months the client ran with us before cancelling. (Whenever someone cancels with us we choose from a list of cancellation reasons within our CRM.) The rows would be the type of marketing channel that the client was running. By clustering this data we would be able to see what types of cancellations to look for at various stages of a customer life cycle, broken down by each marketing channel.Does this problem make sense for Unsupervised Learning and if so do I need to add more features for it or is two enough? Thanks for taking the time to read this!Churn prediction is a supervised learning problem. Clustering could be used as a pre-processing step.I see. Could you expand on what you mean by clustering being used as a pre-processing step?Yes, as you describe, you could group customers based on behavior in an unsupervised way, then fit a model on each group or use group membership as an input to a supervised learning model.It may or may not be helpful, depending on the complexity of the problem and chosen model, e.g. most supervised learning models would do something like this anyway.Ok so outside of the part where I talk about using the Unsupervised Model to predict churn everything else I said would work for Unsupervised Learning? (The features/rows I outlined)It is impossible to know what the most useful features will be. I recommend running some experiments to see what works for your dataset.This might give you ideas about what data to collect:
http://machinelearningmastery.com/how-to-define-your-machine-learning-problem/interesting post. now suggest me algorithms in unsupervised learning to detect malicious/phishing url and legitimate url.That sounds like a supervised learning problem.Thank you so much for such amazing post, very easy understand ……Thank YouYou’re welcome!brilliant read, but i am stuck on something; is it possible to append data on supervised learning models?Thanks!Sure, you can update or refit the model any time you want.Hi, I have to predict student performance of a specific class and i collected all other demographic and previous class data of students. So in this case either i apply supervised or unsupervised learning algorithm.It sounds like supervised learning, this framework will help:
http://machinelearningmastery.com/how-to-define-your-machine-learning-problem/which one is better supervised and unsupervised machine learning?Invalid dichotomy. They are two different classes of technique for solving different problems.If you had to chose one to study that would be most useful “at work”, it would be: supervised learning.Hi Jason,Well structured write that has finally cleared some misconceptions. I wanted to find out where future predictions will fall under.So say I had a variable, Y_p and 3 input variables X1, X2, X3, in my data set but I wanted to predict a future Y value, let’s call it Y_f. Would this be a supervised or unsupervised problem?For example, Y_p could be my current speed, X1, X2 and X3 could be weight, height, age and then Y_f would be the predicted (future speed) after a given period tLooking forward to your response.Thanks.Supervised.Thank you. What algorithms would be best suited to this problem? Given it is a regression problem. I find it a bit boggling in the sense that from my understanding of supervised, the target variable is usually known in the historic data (train set). however, for this problem, the target variable in the historic data isn’t known so perhaps you could point me in the direction where I can really understand why it is still a supervised learning problem and what algorithms best tackle this problem.Good question this will help:
https://machinelearningmastery.com/faq/single-faq/what-algorithm-config-should-i-useHi Jason,Is there any algorithm out there which can perform unsupervised multiclass multi label problems? I know that Kmeans can be used for unsupervised multiclass problems. Is there any way they can be made to work on multi class multi label problems?No. Classification is a supervised learning problem, not unsupervised.Kmeans is not aware of classes, it is not a classification algorithm. It is a clustering algorithm and groups data into the number centers you specify.Hi Jason,
Does Knn require some initial labeled data, on which it can create clusters, or is it done by some other technique?Yes, the model requires a good representative labeled dataset for “training”.A simple and clear explanation. Thank youYou’re welcome!Thank you for this post.
I would like to know how can I train and test in unsupervised learning for image dataset: during training all the dataset is labeled and during test how datasets should be (should i get dataset with masks or only normal dataset)?
Thank youSorry, I don’t have examples of unsupervised learning.May I do the clustering on the image data.I guess so, you may need custom techniques designed for image data.i have doubt can u pls tell is this a supervised (classification) or unsupervised Predicting if a new image has cat or dog based on the historical data of other images of cats and dogs, where you are supplied the information about which image is cat or dogSupervised.Comment Name (required) Email (will not be published) (required) Website 

Δdocument.getElementById( "ak_js" ).setAttribute( "value", ( new Date() ).getTime() );Welcome!
I'm Jason Brownlee PhD 
and I help developers get results with machine learning.
Read moreThe Machine Learning Algorithms EBook is where you'll find the Really Good stuff.© 2021 Machine Learning Mastery. All Rights Reserved.
LinkedIn |
Twitter |
Facebook |
Newsletter |
RSSPrivacy | 
Disclaimer | 
Terms | 
Contact |
Sitemap |
Search