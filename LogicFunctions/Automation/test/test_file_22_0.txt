In information technology (IT), an artificial neural network (ANN) is a system of hardware and/or software patterned after the operation of neurons in the human brain. ANNs -- also called, simply, neural networks -- are a variety of deep learning technology, which also falls under the umbrella of artificial intelligence, or AI.Commercial applications of these technologies generally focus on solving complex signal processing or pattern recognition problems. Examples of significant commercial applications since 2000 include handwriting recognition for check processing, speech-to-text transcription, oil-exploration data analysis, weather prediction and facial recognition.The history of artificial neural networks goes back to the early days of computing. In 1943, mathematicians Warren McCulloch and Walter Pitts built a circuitry system intended to approximate the functioning of the human brain that ran simple algorithms.It wasn't until around 2010 that research picked up again. The big data trend, where companies amass vast troves of data, and parallel computing gave data scientists the training data and computing resources needed to run complex artificial neural networks. In 2012, a neural network was able to beat human performance at an image recognition task as part of the ImageNet competition. Since then, interest in artificial neural networks as has soared and the technology continues to improve.This article is part ofDownload this entire guide for FREE now!An ANN usually involves a large number of processors operating in parallel and arranged in tiers. The first tier receives the raw input information -- analogous to optic nerves in human visual processing. Each successive tier receives the output from the tier preceding it, rather than the raw input -- in the same way neurons further from the optic nerve receive signals from those closer to it. The last tier produces the output of the system.Each processing node has its own small sphere of knowledge, including what it has seen and any rules it was originally programmed with or developed for itself. The tiers are highly interconnected, which means each node in tier n will be connected to many nodes in tier n-1 -- its inputs -- and in tier n+1, which provides input data for those nodes. There may be one or multiple nodes in the output layer, from which the answer it produces can be read.Artificial neural networks are notable for being adaptive, which means they modify themselves as they learn from initial training and subsequent runs provide more information about the world. The most basic learning model is centered on weighting the input streams, which is how each node weights the importance of input data from each of its predecessors. Inputs that contribute to getting right answers are weighted higher.Typically, an ANN is initially trained or fed large amounts of data. Training consists of providing input and telling the network what the output should be. For example, to build a network that identifies the faces of actors, the initial training might be a series of pictures, including actors, non-actors, masks, statuary and animal faces. Each input is accompanied by the matching identification, such as actors' names or "not actor" or "not human" information. Providing the answers allows the model to adjust its internal weightings to learn how to do its job better.For example, if nodes David, Dianne and Dakota tell node Ernie the current input image is a picture of Brad Pitt, but node Durango says it is Betty White, and the training program confirms it is Pitt, Ernie will decrease the weight it assigns to Durango's input and increase the weight it gives to that of David, Dianne and Dakota.In defining the rules and making determinations -- that is, the decision of each node on what to send to the next tier based on inputs from the previous tier -- neural networks use several principles. These include gradient-based training, fuzzy logic, genetic algorithms and Bayesian methods. They may be given some basic rules about object relationships in the data being modeled.For example, a facial recognition system might be instructed, "Eyebrows are found above eyes," or "Moustaches are below a nose. Moustaches are above and/or beside a mouth." Preloading rules can make training faster and make the model more powerful sooner. But it also builds in assumptions about the nature of the problem, which may prove to be either irrelevant and unhelpful or incorrect and counterproductive, making the decision about what, if any, rules to build in very important.Further, the assumptions people make when training algorithms cause neural networks to amplify cultural biases. Biased data sets are an ongoing challenge in training systems that find answers on their own by recognizing patterns in data. If the data feeding the algorithm isn't neutral -- and almost no data is -- the machine propagates bias.Neural networks are sometimes described in terms of their depth, including how many layers they have between input and output, or the model's so-called hidden layers. This is why the term neural network is used almost synonymously with deep learning. They can also be described by the number of hidden nodes the model has or in terms of how many inputs and outputs each node has. Variations on the classic neural network design allow various forms of forward and backward propagation of information among tiers.Specific types of artificial neural networks include:Advantages of artificial neural networks include:The disadvantages of ANNs include:Image recognition was one of the first areas to which neural networks were successfully applied, but the technology uses have expanded to many more areas, including:These are just a few specific areas to which neural networks are being applied today. Prime uses involve any process that operates according to strict rules or patterns and has large amounts of data. If the data involved is too large for a human to make sense of in a reasonable amount of time, the process is likely a prime candidate for automation through artificial neural networks.For more on how various types of neural networks differ from each other and how businesses are using them, follow the links here to the rest of our coverage on the topic.The vendor recently upgraded its products to the latest version of Log4j's software while also adding a new viewing experience ...Predicted trends for 2022 include rising adoption of automated insight delivery and augmented analytics capabilities, including ...The top business benefits of embedded analytics and BI include improving sales, gaining competitive advantages and getting ...Rather than feel the wave of top tech trends of 2022 wash over them, CIOs should focus on business goals to guide their emerging ...With the tech talent shortage in full force, IT talent development is critical for every organization. Learn the essentials of ...IT leaders who bypass strategy and go straight to selecting automation tools are courting failure. Here's how to move toward ...Bringing a big data initiative to fruition requires an array of data skills and best practices. Here are 10 big data challenges ...Thomas Mazzaferro, chief data officer at Western Union, outlines the financial services company's approach to cloud data ...NoSQL graph databases focus on the relationships between pieces of data. Two common frameworks bring different advantages and ...In this Q&A, Unit4's Claus Jepsen calls cloud-based composable ERPs, which enable companies to pick and choose the applications ...A century-old Italian maker of high-end coffee machines used digital twins to virtually eliminate physical prototypes and set the...Executives from managed service provider Inoapps explain how outsourcing ERP infrastructure can buy time to plan an eventual ...All Rights Reserved, 
			Copyright 2018 - 2022, TechTarget
			
				
					Privacy Policy
				
                
				
					Cookie Preferences 
                    
				
				
					Do Not Sell My Personal Info
				