Sign inVishnu Vijayan PVAug 2, 2020·5 min readReinforcement learning (RL) is learning what to do, given a situation and a set of possible actions to choose from, in order to maximize a reward. The learner, which we will call agent, is not told what to do, he must discover this by himself through interacting with the environment. The goal is to choose its actions in such a way that the cumulative reward is maximized. So, choosing the best reward now, might not be the best decision, in the long run. That is greedy approaches might not be optimal.Reinforcement Learning is an approach where an agent learns how to behave in a environment by performing actions and seeing the results. Reinforcement learning is connected to applications for which the algorithm must make decisions and the decisions bear consequences. The goal is defined by maximisation of expected cumulative reward.The algorithm presents a state, depending on the input data in which a user rewards or punishes the algorithm for the action the algorithm took. The algorithm learns from the reward/punishment and updates itself, this continues.• States: The observation, the agent does on the environment after performing an action• Action: An action that the agent performs on the environment based on its observation• Reward: The feedback the agent receives based on the action it performed. If the feedback is positive, it receives a reward and if the feedback is negative, it receives a punishment.There is an agent and an environment. The environment gives the agent a state. The agent chooses an action and receives a reward from the environment along with the new state. This learning process continues until the goal is achieved or some other condition is met.There are three approaches to solving Reinforcement Learning Problems:A policy function is learned which helps in mapping each state to the best action.Getting deep into policies, we further divide policies into two types:2. Value-Based Approach: In value-based RL, the goal of the agent is to optimize the value function V(s)which is defined as a function that tells us the maximum expected future reward the agent shall get at each state.The value of each state is the total amount of the reward an RL agent can expect to collect over the future, from a particular state.The agent will use the above value function to select which state to choose at each step. The agent will always take the state with the biggest value.In the below example, we see that at each step, we will take the biggest value to achieve our goal: 1 ➡ 3 ➡ 4 ➡ 6 so on…3. Model-Based Approach: In model-based RL, the environment is modeled. This means a model of the behaviour of the environment is created.The problem is each environment will need a different model representation.Markov Decision Process (MDP) is mathematical formulations of the RL problem. They satisfy the Markov property:Markov Property — the current state completely represents the state of the environment (world). That is, the future depends only on the present.In an MDP, there is a decision maker, called an agent, that interacts with the environment it’s placed in. These interactions occur sequentially over time. At each time step, the agent will get some representation of the environment’s state. Given this representation, the agent selects an action to take. The environment is then transitioned into a new state, and the agent is given a reward as a consequence of the previous action.Components of an MDP:• Agent• Environment• State• Action• RewardThis process of selecting an action from a given state, transitioning to a new state, and receiving a reward happens sequentially over and over again, which creates something called a trajectory that shows the sequence of states, actions, and rewards.Throughout this process, it is the agent’s goal to maximize the total amount of rewards that it receives from taking actions in given states. This means that the agent wants to maximize not just the immediate reward, but the cumulative rewards it receives over time.In an MDP, we have a set of states S, a set of actions A, and a set of rewards R. We’ll assume that each of these sets has a finite number of elements.At each time step t=0,1,2, ⋯ the agent receives some representation of the environment’s state St∈S. Based on this state, the agent selects an action At∈A. This gives us the state-action pair (St,At).Time is then incremented to the next time step t+1t+1, and the environment is transitioned to a new state St+1∈SSt+1∈S. At this time, the agent receives a numerical reward Rt+1∈RRt+1∈R for the action At taken from state St.We can think of the process of receiving a reward as an arbitrary function ff that maps state-action pairs to rewards. At each time t, we haveThe trajectory representing the sequential process of selecting an action from a state, transitioning to a new state, and receiving a reward can be represented as