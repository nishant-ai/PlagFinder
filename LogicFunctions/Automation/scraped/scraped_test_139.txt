While machine learning sounds highly technical, an introduction to the statistical methods involved quickly brings it within reach. In this article, Toptal Freelance Software Engineer Vladyslav Millier explores basic supervised machine learning algorithms and scikit-learn, using them to predict survival rates for Titanic passengers.The main goal of this reading is to understand enough statistical methodology to be able to leverage the machine learning algorithms in Pythonâ€™s scikit-learn library and then apply this knowledge to solve a classic machine learning problem.The first stop of our journey will take us through a brief history of machine learning. Then we will dive into different algorithms. On our final stop, we will use what we learned to solve the Titanic Survival Rate Prediction Problem.Some disclaimers:With that noted, letâ€™s dive in!As soon as you venture into this field, you realize that machine learning is less romantic than you may think. Initially, I was full of hopes that after I learned more I would be able to construct my own Jarvis AI, which would spend all day coding software and making money for me, so I could spend whole days outdoors reading books, driving a motorcycle, and enjoying a reckless lifestyle while my personal Jarvis makes my pockets deeper. However, I soon realized that the foundation of machine learning algorithms is statistics, which I personally find dull and uninteresting. Fortunately, it did turn out that â€œdullâ€� statistics have some very fascinating applications.You will soon discover that to get to those fascinating applications, you need to understand statistics very well. One of the goals of machine learning algorithms is to find statistical dependencies in supplied data.The supplied data could be anything from checking blood pressure against age to finding handwritten text based on the color of various pixels.That said, I was curious to see if I could use machine learning algorithms to find dependencies in cryptographic hash functions (SHA, MD5, etc.)â€”however, you canâ€™t really do that because proper crypto primitives are constructed in such a way that they eliminate dependencies and produce significantly hard-to-predict output. I believe that, given an infinite amount of time, machine learning algorithms could crack any crypto model.Unfortunately, we donâ€™t have that much time, so we need to find another way to efficiently mine cryptocurrency.  How far have we gotten up until now?The roots of machine learning algorithms come from Thomas Bayes, who was English statistician who lived in the 18th century. His paper An Essay Towards Solving a Problem in the Doctrine of Chances underpins Bayesâ€™ Theorem, which is widely applied in the field of statistics.In the 19th century, Pierre-Simon Laplace published ThÃ©orie analytique des probabilitÃ©s, expanding on the work of Bayes and defining what we know of today as Bayesâ€™ Theorem. Shortly before that, Adrien-Marie Legendre had described the â€œleast squaresâ€� method, also widely used today in supervised learning.The 20th century is the period when the majority of publicly known discoveries have been made in this field. Andrey Markov invented Markov chains, which he used to analyze poems. Alan Turing proposed a learning machine that could become artificially intelligent, basically foreshadowing genetic algorithms. Frank Rosenblatt invented the Perceptron, sparking huge excitement and great coverage in the media.But then the 1970s saw a lot of pessimism around the idea of AIâ€”and thus, reduced fundingâ€”so this period is called an AI winter. The rediscovery of backpropagation in the 1980s caused a resurgence in machine learning research. And today, itâ€™s a hot topic once again.The late Leo Breiman distinguished between two statistical modeling paradigms: Data modeling and algorithmic modeling. â€œAlgorithmic modelingâ€� means more or less the machine learning algorithms like the random forest.Machine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long prehistory in statistics. He also suggested data science as a placeholder term for the overall problem that machine learning specialists and statisticians are both implicitly working on.The machine learning field stands on two main pillars called supervised learning and unsupervised learning. Some people also consider a new field of studyâ€”deep learningâ€”to be separate from the question of supervised vs. unsupervised learning.Supervised learning is when a computer is presented with examples of inputs and their desired outputs. The goal of the computer is to learn a general formula which maps inputs to outputs. This can be further broken down into:In contrast, unsupervised learning is when no labels are given at all and itâ€™s up to the algorithm to find the structure in its input. Unsupervised learning can be a goal in itself when we only need to discover hidden patterns.Deep learning is a new field of study which is inspired by the structure and function of the human brain and based on artificial neural networks rather than just statistical concepts. Deep learning can be used in both supervised and unsupervised approaches.In this article, we will only go through some of the simpler supervised machine learning algorithms and use them to calculate the survival chances of an individual in tragic sinking of the Titanic. But in general, if youâ€™re not sure which algorithm to use, a nice place to start is scikit-learnâ€™s machine learning algorithm cheat-sheet.Perhaps the easiest possible algorithm is linear regression. Sometimes this can be graphically represented as a straight line, but despite its name, if thereâ€™s a polynomial hypothesis, this line could instead be a curve. Either way, it models the relationships between scalar dependent variable $y$ and one or more explanatory values denoted by $x$.In laypersonâ€™s terms, this means that linear regression is the algorithm which learns the dependency between each known $x$ and $y$, such that later we can use it to predict $y$ for an unknown sample of $x$.In our first supervised learning example, we will use a basic linear regression model to predict a personâ€™s blood pressure given their age. This is a very simple dataset with two meaningful features: Age and blood pressure.As already mentioned above, most machine learning algorithms work by finding a statistical dependency in the data provided to them. This dependency is called a hypothesis and is usually denoted by $h(\theta)$.To figure out the hypothesis, letâ€™s start by loading and exploring the data.[<matplotlib.lines.Line2D at 0x11424b828>]On the chart above, every blue dot represents our data sample and the blue line is the hypothesis which our algorithm needs to learn. So what exactly is this hypothesis anyway?In order to solve this problem, we need to learn the dependency between $x$ and $y$, which is denoted by $y = f(x)$. Therefore $f(x)$ is the ideal target function. The machine learning algorithm will try to guess the hypothesis function $h(x)$ that is the closest approximation of the unknown $f(x)$.The simplest possible form of hypothesis for the linear regression problem looks like this: $h_\theta(x) = \theta_0 + \theta_1 * x$. We have a single input scalar variable $x$ which outputs a single scalar variable $y$, where $\theta_0$ and $\theta_1$ are parameters which we need to learn. The process of fitting this blue line in the data is called linear regression. It is important to understand that we have only one input parameter $x_1$; however, a lot of hypothesis functions will also include the bias unit ($x_0$). So our resulting hypothesis has a form of $h_\theta(x) = \theta_0 * x_0 + \theta_1 * x_1$. But we can avoid writing $x_0$ because itâ€™s almost always equal to 1.Getting back to the blue line. Our hypothesis looks like $h(x) = 84 + 1.24x$, which means that $\theta_0 = 84$ and $\theta_1 = 1.24$. How can we automatically derive those $\theta$ values?We need to define a cost function. Essentially, what cost function does is simply calculates the root mean square error between the model prediction and the actual output.For example, our hypothesis predicts that for someone who is 48 years old, their blood pressure should be $h(48) = 84 + 1.24 * 48 = 143mmHg$; however, in our training sample, we have the value of $130 mmHg$. Therefore the error is $(143 - 130)^2 = 169$. Now we need to calculate this error for every single entry in our training dataset, then sum it together ($\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2$) and take the mean value out of that.This gives us a single scalar number which represents the cost of the function. Our goal is to find $\theta$ values such that the cost function is the lowest; in the other words, we want to minimize the cost function. This will hopefully seem intuitive: If we have a small cost function value, this means that the error of prediction is small as well.J(Theta) = 1901.95Now, we need to find such values of $\theta$ such that our cost function value is minimal. But how do we do that?There are several possible algorithms, but the most popular is gradient descent. In order to understand the intuition behind the gradient descent method, letâ€™s first plot it on the graph. For the sake of simplicity, we will assume a simpler hypothesis $h(\theta) = \theta_1 * x$. Next, we will plot a simple 2D chart where $x$ is the value of $\theta$ and $y$ is the cost function at this point.The cost function is convex, which means that on the interval $[a, b]$ there is only one minimum. Which again means that the best $\theta$ parameters are at the point where the cost function is minimal.Basically, gradient descent is an algorithm that tries to find the set of parameters which minimize the function. It starts with an initial set of parameters and iteratively takes steps in the negative direction of the function gradient.If we calculate the derivative of a hypothesis function at a specific point, this will give us a slope of the tangent line to the curve at that point. This means that we can calculate the slope at every single point on the graph.The way the algorithm works is this:Now, the convergence condition depends on the implementation of the algorithm. We may stop after 50 steps, after some threshold, or anything else.The local minimum occurs at 4.712194We will not implement those algorithms in this article. Instead, we will utilize the widely adopted scikit-learn, an open-source Python machine learning library. It provides a lot of very useful APIs for different data mining and machine learning problems.<matplotlib.figure.Figure at 0x120fae1d0>When working with data for machine learning problems, it is important to recognize different types of data. We may have numerical (continuous or discrete), categorical, or ordinal data.Numerical data has meaning as a measurement. For example, age, weight, number of bitcoins that a person owns, or how many articles the person can write per month. Numerical data can be further broken down into discrete and continuous types.Categorical data represent values such as personâ€™s gender, marital status, country, etc. This data can take numerical value, but those numbers have no mathematical meaning. You cannot add them together.Ordinal data can be a mix of the other two types, in that categories may be numbered in a mathematically meaningful way. A common example is ratings: Often we are asked to rate things on a scale of one to ten, and only whole numbers are allowed. While we can use this numericallyâ€”e.g., to find an average rating for somethingâ€”we often treat the data as if it were categorical when it comes to applying machine learning methods to it.Linear regression is an awesome algorithm which helps us to predict numerical values, e.g., the price of the house with the specific size and number of rooms. However, sometimes, we may also want to predict categorical data, to get answers to questions like:Or even:All these questions are specific to the classification problem. And the simplest classification algorithm is called logistic regression, which is eventually the same as linear regression except that it has a different hypothesis.First of all, we can reuse the same linear hypothesis $h_\theta(x) = \theta^T X$ (this is in vectorized form). Whereas linear regression may output any number in the interval $[a, b]$, logistic regression can only output values in $[âˆ’1, 1]$, which is the probability of the object falling in a given category or not.Using a sigmoid function, we can convert any numerical value to represent a value on the interval $[âˆ’1, 1]$.Now, instead of $x$, we need to pass an existing hypothesis and therefore we will get:After that, we can apply a simple threshold saying that if the hypothesis is greater than zero, this is a true value, otherwise false.This means that we can use the same cost function and the same gradient descent algorithm to learn a hypothesis for logistic regression.In our next machine learning algorithm example, we will advise the pilots of the space shuttle whether or not they should use automatic or manual landing control. We have a very small datasetâ€”15 samplesâ€”which consists of six features and the ground truth.In machine learning algorithms, the term â€œground truthâ€� refers to the accuracy of the training setâ€™s classification for supervised learning techniques.Our dataset is complete, meaning that there are no missing features; however, some of the features have a â€œ*â€� instead of the category, which means that this feature does not matter. We will replace all such asterisks with zeroes.Score of our model is 73.33%In the previous example, we validated the performance of our model using the learning data. However, is this now a good option, given that our algorithm can either underfit of overfit the data? Letâ€™s take a look at the simpler example when we have one feature which represents the size of a house and another which represents its price.The machine learning algorithm model is underfitting if it can generalize neither the training data nor new observations. In the example above, we use a simple linear hypothesis which does not really represent the actual training dataset and will have very poor performance. Usually, underfitting is not discussed as it can be easily detected given a good metric.If our algorithm remembers every single observation it was shown, then it will have poor performance on new observations outside of the training dataset. This is called overfitting. For example, a 30th-degree polynomial model passes through the most of the points and has a very good score on the training set, but anything outside of that would perform badly.Our dataset consists of one feature and is simple to plot in 2D space; however, in real life, we may have datasets with hundreds of features, which makes them impossible to plot visually in Euclidean space. What other options do we have in order to see if the model is underfitting or overfitting?Itâ€™s time to introduce you to the concept of the learning curve. This is a simple graph that plots the mean squared error over the number of training samples.In learning materials you will usually see graphs similar to these:However, in real life, you may not get such a perfect picture. Letâ€™s plot the learning curve for each of our models.In our simulated scenario, the blue line, which represents the training score, seems like a straight line. In reality, it still slightly decreasesâ€”you can actually see this in the first-degree polynomial graph, but in the others itâ€™s too subtle to tell at this resolution. We at least clearly see that there is a huge gap between learning curves for training and test observations with a â€œhigh biasâ€� scenario.On the â€œnormalâ€� learning rate graph in the middle, you can see how training score and test score lines come together.And on the â€œhigh varianceâ€� graph, you can see that with a low number of samples, the test and training scores are very similar; however, when you increase the number of samples, the training score remains almost perfect while the test score grows away from it.We can fix underfitting models (also called models with high bias) if we use a non-linear hypothesis, e.g., the hypothesis with more polynomial features.Our overfitting model (high variance) passes through every single example it is shown; however, when we introduce test data, the gap between learning curves widens. We can use regularization, cross-validation, and more data samples to fix overfitting models.One of the common practices to avoid overfitting is to hold onto part of the available data and use it as a test set. However, when evaluating different model settings, such as the number of polynomial features, we are still at risk of overfitting the test set because parameters can be tweaked to achieve the optimal estimator performance and, because of that, our knowledge about the test set can leak into the model. To solve this problem, we need to hold onto one more part of the dataset, which is called the â€œvalidation set.â€� Training proceeds on the training set and, when we think that weâ€™ve achieved the optimal model performance, we can make a final evaluation utilizing the validation set.However, by partitioning the available data into three sets, we dramatically reduce the number of samples that can be used for training the models, and the results can depend on a particular random choice for the training-validation pair of sets.One solution to this problem is a procedure called cross-validation. In standard $k$-fold cross-validation, we partition the data into $k$ subsets, called folds. Then, we iteratively train the algorithm on $k-1$ folds while using the remaining fold as the test set (called the â€œholdout foldâ€�).Cross-validation allows you to tune parameters with only your original training set. This allows you to keep your test set as a truly unseen dataset for selecting your final model.There are a lot more cross-validation techniques, like leave P out, stratified $k$-fold, shuffle and split, etc. but theyâ€™re beyond the scope of this article.This is another technique that can help solve the issue of model overfitting. Most of the datasets have a pattern and some noise. The goal of the regularization is to reduce the influence of the noise on the model.There are three main regularization techniques: Lasso, Tikhonov, and elastic net.L1 regularization (or Lasso regularization) will select some features to shrink to zero, such that they will not play any role in the final model. L1 can be seen as a method to select important features.L2 regularization (or Tikhonov regularization) will force all features to be relatively small, such that they will provide less influence on the model.Elastic net is the combination of L1 and L2.Feature scaling is also an important step while preprocessing the data. Our dataset may have features with values $[-\infty, \infty]$ and other features with a different scale. This is a method to standardize the ranges of independent values.Feature scaling is also an important process to improve the performance of the learning models. First of all, gradient descent will converge much faster if all of the features are scaled to the same norm. Also, a lot of algorithmsâ€”for example, support vector machines (SVM)â€”work by calculating the distance between two points and if one of the features has broad values, then the distance will be highly influenced by this feature.SVM is yet another broadly popular machine learning algorithm which can be used for classification and regression problems. In SVM, we plot each observation as a point in $n$-dimensional space where $n$ is the number of features we have. The value of each feature is the value of particular coordinates. Then, we try to find a hyperplane that separates two classes well enough.After we identify the best hyperplane, we want to add margins, which would separate the two classes further.SVM is very effective where the number of features is very high or if the number of features is larger then the number of data samples. However, since SVM operates on a vector basis, it is crucial to normalize the data prior the usage.Neural network algorithms are probably the most exciting field of machine learning studies. Neural networks try to mimic how the brainâ€™s neurons are connected together.This is how a neural network looks. We combine a lot of nodes together where each node takes a set of inputs, apply some calculations on them, and output a value.There are a huge variety of neural network algorithms for both supervised and unsupervised learning. Neural networks can be used to drive autonomous cars, play games, land airplanes, classify images, and more.The RMS Titanic was a British passenger liner that sank in the North Atlantic Ocean on April 15th, 1912 after it collided with an iceberg. There were about 2,224 crew and passengers, and more than 1,500 died, making it one of the deadliest commercial maritime disasters of all time.Now, since we understand the intuition behind the most basic machine learning algorithms used for classification problems, we can apply our knowledge to predict the survival outcome for those on board the Titanic.Our dataset will be borrowed from the Kaggle data science competitions platform.Our first step would be to load and explore the data. We have 891 test records; each record has the following structure:This dataset contains both numerical and categorical data. Usually, it is a good idea to dive deeper into the data and, based on that, come up with assumptions. However, in this case, we will skip this step and go straight to predictions.At this point, we will rank different types of machine learning algorithms in Python by using scikit-learn to create a set of different models. It will then be easy to see which one performs the best.For every single model, we will use $k$-fold validation.Ok, so our experimental research says that the SVM classifier with a radial basis function (RBF) kernel performs the best. Now, we can serialize our model and re-use it in production applications.Machine learning is not complicated, but itâ€™s a very broad field of study, and it requires knowledge of math and statistics in order to grasp all of its concepts.Right now, machine learning and deep learning are among the hottest topics of discussion in Silicon Valley, and are the bread and butter of almost every data science company, mainly because they can automate many repetitive tasks including speech recognition, driving vehicles, financial trading, caring for patients, cooking, marketing, and so on.Now you can take this knowledge and solve challenges on Kaggle.This was a very brief introduction to supervised machine learning algorithms. Luckily, there are a lot of online courses and information about machine learning algorithms. I personally would recommend starting with Andrew Ngâ€™s course on Coursera.Machine learning algorithms form models automatically using statistical analysis, in contrast to traditional, hard-coded algorithms. This allows them to evolve over time as they look for patterns in data and make predictions as to their classification.The applications of machine learning are almost limitless. It can be used for everything from simple weather prediction and data clustering to complex feature learning; autonomous driving and flying; image, speech, and video recognition; search and recommendation engines; patient diagnosis; and more.Supervised classification needs labels for training data: One picture is a cat, the other is a dog. Unsupervised classification is where the algorithm finds common traits and separates data itself. It will not explicitly tell us that the image is a cat, but it will be able to separate cats from dogs.Supervised learning is where you explicitly tell to the algorithm what the right answer is, so the algorithm can learn and can predict the answer for previously unseen data. Unsupervised learning is where the algorithm has to figure out the answer on its own.The best place to start learning about machine learning is to watch Andrew's Ng course on Coursera, linked in the resources at the end of the article. From there, start taking challenges on Kaggle to develop better intuition about different frameworks and approaches.There are a lot of factors to consider when choosing the right algorithm: the size of the dataset, the nature of the data, speed vs accuracy, etc. Until you develop your own intuition, you can use existing cheatsheets like the one scikit-learn provides.TagsFreelancer? Find your next job.Vlad MillerFreelance Software EngineerAbout the authorVlad is a self-motivated and product-oriented software engineer with work experience in several programming languages and cloud platforms. The majority of his experience lies with full-stack web application development and cloud architecture and management.World-class articles, delivered weekly.Subscription implies consent to our privacy policyThank you!Check out your inbox to confirm your invite.See our related talentFreelancer? Find your next job.Hire the authorVlad MillerFreelance Software EngineerWorld-class articles, delivered weekly.Subscription implies consent to our privacy policyThank you!Check out your inbox to confirm your invite.World-class articles, delivered weekly.Subscription implies consent to our privacy policyThank you!Check out your inbox to confirm your invite.Join the ToptalÂ® community.
