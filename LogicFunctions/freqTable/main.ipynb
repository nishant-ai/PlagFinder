{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8386b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "\n",
    "punctuation += \"’\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c830813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openFile(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fd15aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency(text):\n",
    "    paragraph = text\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(paragraph)\n",
    "    filtered_tokens = [word for word in word_tokens if (not word.lower() in stop_words) and (not word.lower() in punctuation)]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_tokens =[lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "\n",
    "    counted = Counter(filtered_tokens)\n",
    "    counted_2= Counter(ngrams(filtered_tokens,2))\n",
    "    counted_3= Counter(ngrams(filtered_tokens,3))\n",
    "\n",
    "    word_freq = pd.DataFrame(counted.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)\n",
    "    word_pairs = pd.DataFrame(counted_2.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)\n",
    "    trigrams = pd.DataFrame(counted_3.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)\n",
    "\n",
    "    return word_freq, word_pairs, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7be289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequencyMatch(df_scraped , df_test):\n",
    "    merged = df_scraped.set_index(\"word\").join(df_test.set_index(\"word\"), lsuffix='_scraped', rsuffix='_test')\n",
    "    merged = merged.fillna(0)\n",
    "    frequency_match = merged.frequency_test*100 / merged.frequency_scraped\n",
    "    merged['frequency_match'] = frequency_match\n",
    "    merged.frequency_match = np.where(merged.frequency_match < 0, 0, merged.frequency_match)\n",
    "    return merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82f22793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringTokenize(text):\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    stringTokens = sent_tokenize(text)\n",
    "    return stringTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17f3b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_match(testTokenSet, scrapeTokenSet):\n",
    "    matchMap = []\n",
    "    for test_token in testTokenSet:\n",
    "        word_freq_test, pair_freq_test, trigram_freq_test = word_frequency(test_token)\n",
    "        for scrape_token in scrapeTokenSet:\n",
    "            word_freq_scrape, pair_freq_scrape, trigram_freq_scrape = word_frequency(scrape_token)\n",
    "            match_table = frequencyMatch(word_freq_test, word_freq_scrape)\n",
    "            match_mean = match_table.frequency_match[1] # mean\n",
    "            if match_mean == 0:\n",
    "                continue\n",
    "            matchMap.append({\n",
    "                'test_token' : test_token,\n",
    "                'scrape_token' : scrape_token,\n",
    "                'similarity' : match_mean\n",
    "            })\n",
    "    matchMap = pd.DataFrame(matchMap)\n",
    "    return matchMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c314b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = openFile('scraped_test_1.txt')\n",
    "w_1, w_2, w_3 = word_frequency(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43dc215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt2 = openFile('scraped_dat_1.txt')\n",
    "v_1, v_2, v_3 = word_frequency(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19c20217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(          word  frequency\n",
       " 3           AI         16\n",
       " 2     nouvelle          8\n",
       " 51   behaviour          7\n",
       " 9       Brooks          6\n",
       " 52           “          6\n",
       " ..         ...        ...\n",
       " 96   exploring          1\n",
       " 97     surface          1\n",
       " 98        Mars          1\n",
       " 99         See          1\n",
       " 238  premature          1\n",
       " \n",
       " [239 rows x 2 columns],\n",
       "              word  frequency\n",
       " 10       computer         12\n",
       " 7    intelligence         10\n",
       " 16          human          9\n",
       " 30       learning          6\n",
       " 94        program          6\n",
       " ..            ...        ...\n",
       " 99           much          1\n",
       " 100      everyday          1\n",
       " 101     knowledge          1\n",
       " 102          hand          1\n",
       " 247       similar          1\n",
       " \n",
       " [248 rows x 2 columns])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_1, v_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c94cbabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    239.000000\n",
       "mean      20.680165\n",
       "std       65.117273\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        0.000000\n",
       "max      600.000000\n",
       "Name: frequency_match, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencyMatch(w_1, v_1).frequency_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0d76639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.680165371587965"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencyMatch(w_1, v_1).frequency_match[1] #mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b624af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencyMatch(w_1, v_1).frequency_match[7] #max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a2c5656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencyMatch(w_1, w_1).frequency_match[2] #std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1882a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    351.0\n",
       "mean       0.0\n",
       "std        0.0\n",
       "min        0.0\n",
       "25%        0.0\n",
       "50%        0.0\n",
       "75%        0.0\n",
       "max        0.0\n",
       "Name: frequency_match, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencyMatch(w_3, v_3).frequency_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d69ae85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = stringTokenize(txt)\n",
    "s2 = stringTokenize(txt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "15d0967c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.4006112766706"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_match(s1, s2).similarity.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f838d8ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'scraped/scraped_test_0.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-75464dffa653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{word_mean}, {word_std}, {word_max}, {pair_mean}, {pair_std}, {pair_max}, {trigram_mean}, {trigram_std}, {trigram_max}, {sent_match}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0minitFeatureExtractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-75464dffa653>\u001b[0m in \u001b[0;36minitFeatureExtractions\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minitFeatureExtractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mscrape_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'scraped/scraped_test_{i}.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mtest_file_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test/test_file_{i}_0.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtest_file_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test/test_file_{i}_1.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-75464dffa653>\u001b[0m in \u001b[0;36mopenFile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopenFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'scraped/scraped_test_0.txt'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6494291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
