{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973f6fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1234f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import unicodedata\n",
    "punctuation += \"''``“”\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8412d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency(text):\n",
    "    paragraph = text\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(paragraph)\n",
    "    filtered_tokens = [word for word in word_tokens if (not word.lower() in stop_words) and (not word.lower() in punctuation)]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_tokens =[lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "\n",
    "    counted = Counter(filtered_tokens)\n",
    "    counted_2= Counter(ngrams(filtered_tokens,2))\n",
    "    counted_3= Counter(ngrams(filtered_tokens,3))\n",
    "\n",
    "    word_freq = pd.DataFrame(counted.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)\n",
    "    word_pairs = pd.DataFrame(counted_2.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)\n",
    "    trigrams = pd.DataFrame(counted_3.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)\n",
    "\n",
    "    return word_freq, word_pairs, trigrams\n",
    "\n",
    "def frequencyMatch(df_scraped , df_test):\n",
    "    merged = df_scraped.set_index(\"word\").join(df_test.set_index(\"word\"), lsuffix='_scraped', rsuffix='_test')\n",
    "    merged = merged.fillna(0)\n",
    "    frequency_match = merged.frequency_test*100/merged.frequency_scraped\n",
    "    merged['frequency_match'] = frequency_match\n",
    "    merged.frequency_match = np.where(merged.frequency_match < 0, 0, merged.frequency_match)\n",
    "    merged.frequency_match = np.where(merged.frequency_match > 100, 100, merged.frequency_match)\n",
    "    merged = merged.drop(merged[merged.frequency_test == 0].index)\n",
    "#     print(merged)\n",
    "    return merged['frequency_match'].mean()\n",
    "\n",
    "def stringTokenize(text):\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    stringTokens = sent_tokenize(text)\n",
    "    return stringTokens\n",
    "\n",
    "def sentence_match(testTokenSet, scrapeTokenSet):\n",
    "    matchMap = []\n",
    "    for test_token in testTokenSet:\n",
    "        word_freq_test, pair_freq_test, trigram_freq_test = word_frequency(test_token)\n",
    "        for scrape_token in scrapeTokenSet:\n",
    "            word_freq_scrape, pair_freq_scrape, trigram_freq_scrape = word_frequency(scrape_token)\n",
    "            match_mean = frequencyMatch(word_freq_test, word_freq_scrape)\n",
    "            matchMap.append({\n",
    "                'test_token' : test_token,\n",
    "                'scrape_token' : scrape_token,\n",
    "                'similarity' : match_mean\n",
    "                })\n",
    "    matchMap = pd.DataFrame(matchMap)\n",
    "    matchMap = matchMap[matchMap['similarity'].notna()]\n",
    "    return matchMap\n",
    "\n",
    "def features(test_text, scrape_text):\n",
    "    word_freq_s, word_pair_s, trigram_s = word_frequency(scrape_text)\n",
    "    string_token_s = stringTokenize(scrape_text)\n",
    "    \n",
    "    word_freq_t, word_pair_t, trigram_t = word_frequency(test_text)\n",
    "    string_token_t = stringTokenize(test_text)\n",
    "    \n",
    "    \n",
    "    word_freq_match = frequencyMatch(word_freq_s, word_freq_t)\n",
    "    word_pair_match = frequencyMatch(word_pair_s, word_pair_t)\n",
    "    trigram_match = frequencyMatch(trigram_s, trigram_t)\n",
    "    sent_match = sentence_match(string_token_t, string_token_s).similarity.mean()\n",
    "    if math.isnan(trigram_match):\n",
    "        trigram_match = 0\n",
    "    if math.isnan(word_pair_match):\n",
    "        word_pair_match = 0\n",
    "    if math.isnan(word_freq_match):\n",
    "        word_freq_match = 0\n",
    "    if math.isnan(sent_match):\n",
    "        sent_match = 0\n",
    "    \n",
    "    paramArray = [[word_freq_match, word_pair_match, trigram_match, sent_match]]\n",
    "    return paramArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7359d775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>category</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g0pA_taska.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g0pA_taskb.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g0pA_taskc.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g0pA_taskd.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g0pA_taske.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file  category task\n",
       "0  g0pA_taska.txt         0    a\n",
       "1  g0pA_taskb.txt         1    b\n",
       "2  g0pA_taskc.txt         0    c\n",
       "3  g0pA_taskd.txt         1    d\n",
       "4  g0pA_taske.txt         0    e"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv('data_set.csv')\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "484c78fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83.8888888888889, 100.0, 0, 88.78205128205128, 0]\n",
      "[80.30303030303031, 0, 0, 100.0, 1]\n",
      "[85.8974358974359, 0, 0, 95.0, 0]\n",
      "[81.25, 0, 0, 100.0, 1]\n",
      "[100.0, 0, 0, 93.18181818181819, 0]\n",
      "[81.96969696969698, 75.0, 0, 90.625, 0]\n",
      "[88.33333333333334, 0, 0, 88.46153846153847, 0]\n",
      "[91.66666666666667, 0, 0, 96.875, 1]\n",
      "[85.55555555555557, 0, 0, 100.0, 0]\n",
      "[100.0, 0, 0, 93.25396825396825, 1]\n",
      "[92.38738738738738, 97.22222222222223, 100.0, 94.96173469387756, 1]\n",
      "[92.85714285714286, 0, 0, 100.0, 0]\n",
      "[100.0, 0, 0, 100.0, 0]\n",
      "[80.55555555555556, 0, 0, 100.0, 1]\n",
      "[91.02564102564104, 100.0, 0, 96.2962962962963, 0]\n",
      "[91.88967136150234, 99.31506849315069, 100.0, 94.32624113475178, 1]\n",
      "[100.0, 0, 0, 100.0, 0]\n",
      "[95.83333333333333, 0, 0, 100.0, 1]\n",
      "[83.33333333333333, 0, 0, 100.0, 0]\n",
      "[100.0, 0, 0, 100.0, 0]\n",
      "[98.85245901639344, 100.0, 100.0, 94.86666666666666, 0]\n",
      "[83.33333333333333, 0, 0, 100.0, 1]\n",
      "[80.0, 0, 0, 100.0, 0]\n",
      "[100.0, 0, 0, 100.0, 0]\n",
      "[90.27777777777779, 100.0, 0, 95.3125, 1]\n",
      "[97.36842105263158, 75.0, 0, 86.7171717171717, 0]\n",
      "[92.85714285714286, 0, 0, 100.0, 1]\n",
      "[91.66666666666667, 0, 0, 100.0, 0]\n",
      "[100.0, 0, 0, 88.88888888888887, 1]\n",
      "[84.89583333333334, 0, 0, 99.10714285714286, 0]\n",
      "[73.26923076923077, 100.0, 0, 98.07692307692308, 0]\n",
      "[90.0, 0, 0, 96.66666666666667, 0]\n",
      "[84.84848484848486, 0, 0, 100.0, 1]\n",
      "[100.0, 0, 0, 95.0, 0]\n",
      "[91.2280701754386, 100.0, 0, 90.40404040404042, 1]\n",
      "[93.8135593220339, 98.64864864864865, 97.36842105263158, 93.42592592592592, 0]\n",
      "[88.8888888888889, 0, 0, 100.0, 1]\n",
      "[76.66666666666667, 0, 0, 93.75, 0]\n",
      "[90.0, 0, 0, 87.5, 0]\n",
      "[100.0, 0, 0, 90.68627450980391, 1]\n",
      "[81.70634920634922, 0, 0, 99.03846153846153, 0]\n",
      "[88.88888888888889, 0, 0, 100.0, 1]\n",
      "[90.9090909090909, 0, 0, 96.42857142857143, 0]\n",
      "[88.33333333333334, 0, 0, 100.0, 1]\n",
      "[100.0, 0, 0, 100.0, 0]\n",
      "[82.12962962962963, 100.0, 0, 96.73913043478261, 0]\n",
      "[100.0, 0, 0, 91.66666666666667, 0]\n",
      "[93.75, 0, 0, 100.0, 1]\n",
      "[85.18518518518519, 0, 0, 100.0, 0]\n",
      "[93.13725490196079, 100.0, 0, 93.96551724137932, 1]\n",
      "[95.15277777777779, 100.0, 100.0, 93.14655172413794, 1]\n",
      "[100.0, 0, 0, 100.0, 0]\n",
      "[100.0, 0, 0, 95.45454545454545, 0]\n",
      "[91.66666666666667, 0, 0, 91.66666666666667, 1]\n",
      "[93.33333333333333, 0, 0, 96.18055555555554, 0]\n",
      "[97.07951070336391, 100.0, 100.0, 96.57604323780795, 1]\n",
      "[81.25, 0, 0, 93.75, 0]\n",
      "[90.47619047619048, 0, 0, 100.0, 1]\n",
      "[92.5925925925926, 0, 0, 94.64285714285714, 0]\n",
      "[87.03703703703704, 0, 0, 93.18181818181819, 0]\n",
      "[93.80952380952381, 100.0, 100.0, 95.9375, 0]\n",
      "[87.03703703703704, 0, 0, 100.0, 1]\n",
      "[91.66666666666667, 0, 0, 93.75, 0]\n",
      "[89.74358974358975, 0, 0, 100.0, 1]\n",
      "[100.0, 0, 0, 90.9090909090909, 0]\n",
      "[82.36111111111111, 100.0, 0, 97.5, 0]\n",
      "[100.0, 0, 0, 100.0, 0]\n",
      "[94.44444444444444, 100.0, 0, 93.18181818181819, 1]\n",
      "[76.66666666666667, 0, 0, 100.0, 0]\n",
      "[81.94444444444444, 100.0, 0, 94.44444444444444, 1]\n",
      "[90.64465408805032, 99.24242424242425, 100.0, 96.04166666666666, 1]\n",
      "[100.0, 0, 0, 100.0, 0]\n",
      "[100.0, 0, 0, 100.0, 0]\n",
      "[100.0, 0, 0, 87.5, 1]\n",
      "[93.93939393939395, 100.0, 0, 95.0, 0]\n",
      "[87.6923076923077, 100.0, 0, 97.0, 0]\n",
      "[95.45454545454545, 0, 0, 95.23809523809526, 0]\n",
      "[87.5, 0, 0, 100.0, 1]\n",
      "[90.27777777777779, 0, 0, 100.0, 0]\n",
      "[93.75, 0, 0, 95.71428571428571, 1]\n",
      "[98.79032258064517, 99.6875, 100.0, 93.54430379746834, 1]\n",
      "[90.0, 0, 0, 100.0, 0]\n",
      "[100.0, 0, 0, 100.0, 0]\n",
      "[84.52380952380953, 0, 0, 100.0, 1]\n",
      "[90.74074074074075, 100.0, 0, 95.37037037037037, 0]\n",
      "[94.51754385964912, 98.61111111111111, 100.0, 96.97222222222223, 0]\n",
      "[88.8888888888889, 0, 0, 100.0, 1]\n",
      "[87.5, 0, 0, 100.0, 0]\n",
      "[92.5925925925926, 0, 0, 100.0, 0]\n",
      "[97.36842105263158, 0, 0, 96.7741935483871, 1]\n",
      "[85.1904761904762, 90.0, 100.0, 93.75661375661375, 1]\n",
      "[84.52380952380953, 0, 0, 100.0, 0]\n",
      "[100.0, 0, 0, 100.0, 1]\n",
      "[83.33333333333333, 0, 0, 95.0, 0]\n",
      "[95.83333333333334, 100.0, 0, 94.62365591397848, 0]\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for file in corpus.file:\n",
    "    with open('corpus/'+file, errors='ignore') as f1:\n",
    "        txt_test = f1.read()\n",
    "        if 'a' in file:\n",
    "            f2 = open('corpus/orig_taska.txt', errors='ignore')\n",
    "            txt_orig = f2.read()\n",
    "        elif 'b' in file:\n",
    "            f2 = open('corpus/orig_taskb.txt', errors='ignore')\n",
    "            txt_orig = f2.read()\n",
    "        elif 'c' in file:\n",
    "            f2 = open('corpus/orig_taskc.txt', errors='ignore')\n",
    "            txt_orig = f2.read()\n",
    "        elif 'd' in file:\n",
    "            f2 = open('corpus/orig_taskd.txt', errors='ignore')\n",
    "            txt_orig = f2.read()\n",
    "        elif 'e' in file:\n",
    "            f2 = open('corpus/orig_taske.txt', errors='ignore')\n",
    "            txt_orig = f2.read()\n",
    "        else:\n",
    "            continue\n",
    "        f2.close()\n",
    "        \n",
    "        feats = features(txt_test, txt_orig)\n",
    "        flag = corpus.category[i]\n",
    "        feats[0].append(flag)\n",
    "        print(feats[0])\n",
    "        i+=1\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "251a66f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.category[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeb01ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
